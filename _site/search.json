[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "BLOG",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nBayes and the Battlefield\n\n\nReasoning Under Uncertainty\n\n\n\n\nbayes\n\n\nwar\n\n\nforecasting\n\n\nrisk-analysis\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2025\n\n\n21 min\n\n\n\n\n\n\n  \n\n\n\n\nParametric Survival Analysis: UN Peacekeeping Missions\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nsurvival-analysis\n\n\nwar\n\n\npeacekeeping\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\nProxy Wars - Scraping data from Wikipedia\n\n\n\n\n\n\n\nscraping\n\n\nR\n\n\ncombat\n\n\n\n\nScrape Wikipedia for proxy war data\n\n\n\n\n\n\nAug 8, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nInsurgent (Pairwise) Networks\n\n\n\n\n\n\n\nnetwork analysis\n\n\nR\n\n\ncombat\n\n\n\n\nConflict makes for strange bedfellows\n\n\n\n\n\n\nAug 22, 2022\n\n\n13 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bad ideas executed with vigor",
    "section": "",
    "text": "Welcome to my data science sandbox.\nI’m a former professional dancer and U.S. Army Reservist turned government analyst and data scientist.\nThis is where I come to play with data, test ideas, and explore the intersections of society, the arts, and conflict. I’ve always been intrigued by the passions and dynamics that drive people and systems. This blog is my platform for examining those forces through the lens of data science.\nLet me be clear: I make no claims that anything here would pass academic muster. Think of this more as stumbling into a private studio or workshop. I aim to be thoughtful and thorough, but you won’t find peer-reviewed publications.\nSo poke around. I hope you find something interesting. If it’s useful, steal it. If you have thoughts or suggestions, I’d love to hear them.\nThanks for stopping by."
  },
  {
    "objectID": "posts/new/new_post.html",
    "href": "posts/new/new_post.html",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod <- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds <- dat %>% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit > 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/new/new_post.html#subheading",
    "href": "posts/new/new_post.html#subheading",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "posts/new/new_post.html#columns",
    "href": "posts/new/new_post.html#columns",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "geom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "posts/new/new_post.html#margin-captions",
    "href": "posts/new/new_post.html#margin-captions",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "ggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/proxy-wiki-scrape/index.html",
    "href": "posts/proxy-wiki-scrape/index.html",
    "title": "Proxy Wars - Wiki Scrape",
    "section": "",
    "text": "Image source: Virginia Tech Publishing"
  },
  {
    "objectID": "posts/proxy-wiki-scrape/index.html#proxy-wars",
    "href": "posts/proxy-wiki-scrape/index.html#proxy-wars",
    "title": "Proxy Wars - Wiki Scrape",
    "section": "Proxy wars",
    "text": "Proxy wars\nViolence, conflict, and proxy warfare – all on a blissful, dreamy Sunday afternoon. Actually, I was listening to an NPR discussion on an afternoon drive that referenced the work of the Proxy War Project out of Virginia Tech. As a data geek, the next obvious question is: Gee, where can I find data on that?\nWhat does the general public think of regarding proxy warfare? Is it a rag tag group of thugs funded by a dark agency and left to run around the forest? Or maybe proxy wars are a relic of the cold war? To the contrary, according to War on the Rocks (a super cool site on all things war and conflict), “[e]vents of the last decade suggest the increasing salience of such conflicts.” The blog goes on to say that “[p]roxy wars are poised to be a…significant factor in the evolving strategic environment.”\nSo, as soon as I got home, I jumped online for a quick search for some data. The first thing I found was the Wikipedia page. It most certainly is not the most comprehensive data, but it has a nice feature: A helpful Wiki contributor created a typology of proxy wars. I decided to scrape it to started some initial exploration.\nI’m not intending for this to be a deep dive into coding nor proxy war dynamics. This is just a quick and dirty scrape of Wikipedia data to get a sense of what’s going."
  },
  {
    "objectID": "posts/proxy-wiki-scrape/index.html#scraping-and-cleaning-wikipedia-data",
    "href": "posts/proxy-wiki-scrape/index.html#scraping-and-cleaning-wikipedia-data",
    "title": "Proxy Wars - Wiki Scrape",
    "section": "Scraping and Cleaning Wikipedia Data",
    "text": "Scraping and Cleaning Wikipedia Data\n\n\nCode\n# libraries\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(kableExtra)\nsource(\"my_gg_theme.R\")\n\n# Scrape the tables from the page\nproxy_tables &lt;- \n  read_html('https://en.wikipedia.org/wiki/List_of_proxy_wars') %&gt;%\n  html_table(fill = TRUE) %&gt;% \n  # set names based on Wikipedia names\n  set_names(\n    c(\n      'Caveat',\n      'Series', \n      'Pre-World War I proxy wars', \n      'Inter-war period proxy wars',\n      'Cold War proxy wars',\n      'Modern proxy wars',\n      'Ongoing proxy wars'\n    )) \n\n\nThe Wikipedia page displays the following caveat:\n\nThis article or section appears to be slanted towards recent events. Please try to keep recent events in historical perspective and add more content related to non-recent events. (October 2022)\n\nWikipedia spread the data across a number of tables. To prep the data for plotting, I put them into one big data frame and did some minor cleaning. In particular, the dates were a little jenky. The resulting data looks like:\n\n\nCode\n# data cleaning\nproxy_data_df &lt;- \n  # drop unneeded data\n  proxy_tables[-c(1,2)] %&gt;% \n  map2_df(\n    names(proxy_tables[-c(1,2)]),\n    ~mutate(.x, war_type = .y)) %&gt;% \n  janitor::clean_names() %&gt;% \n  separate(\n    dates, \n    into = c('start_year', 'end_year'), \n    sep = \"–\") %&gt;%\n  mutate(across(\n    start_year:end_year,\n    ~ str_extract(.x, '[0-9]{4}') %&gt;% \n    as.numeric(.x))) %&gt;% \n  mutate(across(\n    c(war, combatant_1, combatant_1, result),\n    ~ str_remove(., \"\\\\[.*\\\\]$\")))\n\nproxy_data_df %&gt;% \n  slice_head(n = 5) %&gt;% \n  knitr::kable() %&gt;%\n  kable_styling(font_size = 7)\n\n\n\n\n\nwar\nstart_year\nend_year\ncombatant_1\ncombatant_2\nresult\nwar_type\n\n\n\n\nEgyptian–Ottoman War\n1839\n1841\nEgypt-aligned powers: Egypt France Spain\nAllied powers: British Empire Austrian Empire Russian Empire Kingdom of Prussia Ottoman Empire\nCompromise\nPre-World War I proxy wars\n\n\nUruguayan Civil War\n1839\n1851\nColorados Unitarian Party Empire of Brazil Italian Legion France Great Britain\nBlancos Argentine Confederation\nColorado victory\nPre-World War I proxy wars\n\n\nMahdist War\n1881\n1899\nBritish Empire  Canada Khedivate of Egypt Belgium  Congo Free State Ethiopian Empire  Italy Supported by: Emirate of Jabal Shammar\nMahdist Sudan Supported by: Ottoman Empire Russian Empire France\nBritish-Egyptian-Italian victory\nPre-World War I proxy wars\n\n\nFirst Samoan Civil War\n1886\n1894\nTamasese German Empire\nMata'afans Supported by: United States\nStalemate\nPre-World War I proxy wars\n\n\nSecond Samoan Civil War\n1898\n1899\nMata'afans German Empire\nSamoa United Kingdom United States\nStalemate\nPre-World War I proxy wars"
  },
  {
    "objectID": "posts/proxy-wiki-scrape/index.html#plotting-war-duration",
    "href": "posts/proxy-wiki-scrape/index.html#plotting-war-duration",
    "title": "Proxy Wars - Wiki Scrape",
    "section": "Plotting war duration",
    "text": "Plotting war duration\nTo avoid rewriting a bunch of code, let’s create a little function to select the war-type of choice and plot the data. (Note, the plotting code was a little long and distracting, so I pushed some of it to a source script.)\n\n\nCode\n# a function to select war type and plot\nwar_plot &lt;- function(df, type) {\n  \n  df %&gt;% \n    filter(\n      war_type == type\n    ) %&gt;% \n    ggplot(aes(\n      x = fct_reorder(war, -start_year),\n      y = start_year)) +\n    geom_segment(aes(\n      xend = war, \n      yend = end_year), \n      color = \"cadetblue\",\n      alpha = 0.75) +\n    # year start\n    geom_point(color = \"orange\", # #E69F00\n               size = 3,\n               alpha = 0.8) +\n    # year end\n    geom_point(aes(\n      y = end_year), \n      color = \"cadetblue\", \n      size = 3,\n      alpha = 0.8) +  \n    \n    coord_flip() +\n    labs(\n      x = \"\", \n      y = \"\",\n      title = type\n      ) +\n    # a sourced function for some trivial formatting \n    quick_gg_theme()\n}\n\n\n\n\nCode\nwar_plot(proxy_data_df, \"Pre-World War I proxy wars\")\nwar_plot(proxy_data_df, 'Inter-war period proxy wars')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwar_plot(proxy_data_df, 'Modern proxy wars')\nwar_plot(proxy_data_df, 'Ongoing proxy wars')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwar_plot(proxy_data_df, 'Cold War proxy wars')"
  },
  {
    "objectID": "posts/proxy-wiki-scrape/index.html#wrap-up",
    "href": "posts/proxy-wiki-scrape/index.html#wrap-up",
    "title": "Proxy Wars - Wiki Scrape",
    "section": "Wrap up",
    "text": "Wrap up\nFirst, that’s a lot o’ proxy wars. Of course, there’s clearly a few problems with the data. Some dates are missing. And I’m suspicious about the ongoing proxy wars. Are they all really still ongoing?\nThe Cold War was fertile ground for proxy wars. I guess it makes sense, right? Cold wars are characterized by indirect conflict, working through proxies, pawns, and agents of mayhem.\nGiven that proxy wars are frequently a means for “agents” to have deniability regarding the havoc they are wrecking, there is likely a large number of unreported/undiscovered agent-proxy relationships.\nIt might be interesting to parse the text-based columns and see which actors are most frequently leveraging proxies, and in what parts of the world…another day…"
  },
  {
    "objectID": "posts/series1/new_post/post.html",
    "href": "posts/series1/new_post/post.html",
    "title": "Dummy post in series",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/series1/new_post/post.html#merriweather",
    "href": "posts/series1/new_post/post.html#merriweather",
    "title": "Dummy post in series",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "posts/series1/new_post/post.html#columns",
    "href": "posts/series1/new_post/post.html#columns",
    "title": "Dummy post in series",
    "section": "",
    "text": "geom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "posts/series1/new_post/post.html#margin-captions",
    "href": "posts/series1/new_post/post.html#margin-captions",
    "title": "Dummy post in series",
    "section": "",
    "text": "ggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/pairwise_network/network_pairwise.html",
    "href": "posts/pairwise_network/network_pairwise.html",
    "title": "Insurgent (Pairwise) Networks",
    "section": "",
    "text": "During the height of the Syrian civil war, many of us watching from afar were puzzled when “moderate” insurgent groups aligned themselves with hardliners or jihadists. The conflict featured hundreds—perhaps thousands—of armed groups forming various kinds of alliances, only to break apart and later reconcile.\nA lot is made of the role of ideology when armchair commanders (like myself) comment on these unfolding disasters. But alliances can also be pragmatic choices. By aggregating combat power, armed groups might increase their chances of survival—even if only to turn on each other at a later stage.\nRecently, I came across a compelling argument in Alliance Formation in Civil Wars by Fotini Christia.1 In this book, Christia states: “It would be natural to suppose that warring groups form alliances based on shared identity considerations—such as Christian groups allying with Christian groups or Muslim groups with their fellow co-religionists—but this is not what we see.”\nWith that motivation in mind, I decided to explore what we could learn from Syrian armed group coalitions. When these groups engaged in battle, they frequently partnered with multiple other factions. This raised a key question for me: Would groups tolerate fighting alongside ideological enemies?\nSo, I focused on pairwise relationships, even within multi-group alliances.\nCode\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(rsvg)\nlibrary(cowplot)\nlibrary(ggtext)\nlibrary(igraph)\nlibrary(devtools)"
  },
  {
    "objectID": "posts/pairwise_network/network_pairwise.html#data",
    "href": "posts/pairwise_network/network_pairwise.html#data",
    "title": "Insurgent (Pairwise) Networks",
    "section": "Data",
    "text": "Data\n\n\n\n\n\n\nA love affair with ACLED data!\n\n\n\nI came across the Armed Conflict Location & Event Data Project (ACLED) a few years ago—and what an amazing resource.\nACLED is a disaggregated data collection, analysis, and crisis mapping project. The ACLED team collects both real-time and historical data on political violence and protest events across nearly 100 countries.\nIn short, it’s an outstanding tool for understanding the dynamics of conflict. .2\n\n\nACLED data is robust, with many columns and coverage spanning multiple years. In this dataset, the actor1 column (along with assoc_actor_1) represents entities that are acting together—typically in some form of violence—against actor2.\nFor this analysis, I’m only concerned with actor1 and assoc_actor_1, since I’m interested in collaborations, not opponents. These columns capture groups that are working together during a reported event.\nLet’s take a quick peek at the data:\n\n\nCode\nread_csv(\"df_acled_syr_2017_2021.csv\") |&gt; \n  select(year, event_date, event_type, actor1, assoc_actor_1) |&gt; \n  filter(!is.na(assoc_actor_1)) |&gt;\n  slice_head(n = 5) |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nevent_date\nevent_type\nactor1\nassoc_actor_1\n\n\n\n\n2021\n2021-02-05\nBattles\nOperation Peace Spring\nJWS: Syrian National Army; Military Forces of Turkey (2016-)\n\n\n2021\n2021-02-05\nBattles\nOpposition Rebels (Syria)\nMilitary Forces of Turkey (2016-)\n\n\n2021\n2021-02-05\nExplosions/Remote violence\nAl Fath Al Mubeen Operation Room\nOpposition Rebels (Syria)\n\n\n2021\n2021-02-05\nBattles\nMilitary Forces of Syria (2000-)\nMilitia (Pro-Government)\n\n\n2021\n2021-02-04\nExplosions/Remote violence\nOpposition Rebels (Syria)\nMilitary Forces of Turkey (2016-)\n\n\n\n\n\nI need select the columns of interest and then filter down to battle related events (filtering out Protests, Riots, and Violence against civilians)\n\n\nCode\ndf_acled_syr &lt;- \n  read_csv(\"df_acled_syr_2017_2021.csv\") |&gt; \n  filter(year %in% 2017:2019,\n         event_type %in% c(\"Explosions/Remote violence\",\n                           \"Battles\",\n                           \"Strategic developments\")) |&gt; \n  select(actor1, assoc_actor_1) |&gt; \n  mutate(actor1 = str_squish(actor1),\n         assoc_actor_1 = str_squish(assoc_actor_1)) \n\n\nHere’s the difficult part. The reduced ACLED data looks akin to this:\n\n\nCode\n(test &lt;- tibble::tibble(\n  actor1 = c(\"A\", \"A\", \"B\"),\n  assoc_actor_1 = c(\"B\", \"B; C\", \"C; D; E\"))) |&gt; \n  knitr::kable()\n\n\n\n\n\nactor1\nassoc_actor_1\n\n\n\n\nA\nB\n\n\nA\nB; C\n\n\nB\nC; D; E\n\n\n\n\n\nFor what I’m interested in, I need to get the pairwise combinations. So, I need to work out and test the code to accomplish this. A huge thanks to Dusty Turner.3 In fact, this chunk is largely thanks to his generosity.\n\n\nCode\n#| # Separate assoc actors to get all pairwise partnerships'\ndf &lt;- \n  test |&gt; \n  # use only actors with multiple associated actors\n  filter(str_detect(assoc_actor_1, \";\")) |&gt;\n  separate_rows(assoc_actor_1, sep = \";\") |&gt;\n  mutate(assoc_actor_1 = str_squish(assoc_actor_1)) |&gt; \n  pivot_longer(actor1:assoc_actor_1) |&gt; \n  select(value) |&gt; \n  distinct(value) |&gt; \n  mutate(value2 = value) |&gt; \n  expand(value, value2) |&gt; \n  filter(value !=value2) |&gt; \n  mutate(helper = str_c(value,value2)) |&gt; \n  rowwise() |&gt; \n  mutate(helper = str_c(str_sort(unlist(str_split(helper, \"\"))),collapse = \"\")) |&gt; \n  distinct(helper,.keep_all = T) |&gt; \n  select(-helper) |&gt; \n  rename(actor1 = value, assoc_actor_1 = value2)\n\n# bind back with actors that don't have multiple assoc actors\ntest |&gt; \n  filter(!str_detect(assoc_actor_1, \";\")) |&gt;\n  bind_rows(df) |&gt; \n  knitr::kable()\n\n\n\n\n\nactor1\nassoc_actor_1\n\n\n\n\nA\nB\n\n\nA\nB\n\n\nA\nC\n\n\nA\nD\n\n\nA\nE\n\n\nB\nC\n\n\nB\nD\n\n\nB\nE\n\n\nC\nD\n\n\nC\nE\n\n\nD\nE\n\n\n\n\n\nThat looks good! Let’s apply it to the ACLED data.\n\n\nCode\n# Separate assoc actors to get all pairwise partnerships'\ndf_separate &lt;- \n  df_acled_syr |&gt; \n  # use only actors with multiple associated actors\n  filter(str_detect(assoc_actor_1, \";\")) |&gt;\n  separate_rows(assoc_actor_1, sep = \";\") |&gt;\n  mutate(assoc_actor_1 = str_squish(assoc_actor_1)) |&gt; \n  pivot_longer(actor1:assoc_actor_1) |&gt; \n  select(value) |&gt; \n  distinct(value) |&gt; \n  mutate(value2 = value) |&gt; \n  expand(value, value2) |&gt; \n  filter(value !=value2) |&gt; \n  mutate(helper = str_c(value,value2)) |&gt; \n  rowwise() |&gt; \n  mutate(helper = str_c(str_sort(unlist(str_split(helper, \"\"))),collapse = \"\")) |&gt; \n  distinct(helper,.keep_all = T) |&gt; \n  select(-helper) |&gt; \n  rename(actor1 = value, assoc_actor_1 = value2)\n\n# bind back with actors that don't have multiple assoc actors\nreshaped_df &lt;- df_acled_syr |&gt; \n  filter(!str_detect(assoc_actor_1, \";\")) |&gt;\n  bind_rows(df_separate)\n\n\nOk, I have to confess. This next part stumped me and took forever. First, a huge shout out to Wikipedia contributors. There are soooo many actors in the dataset. I spend much more time than intended trying to consolidate them, constantly referencing Wikipedia to figure out what’s going on.\nSecond, I needed to clean and consolidate the actors, and then remove the non-opposition groups as well as the non-armed-opposition groups. I tried several ways to make happen much more parsimoniously with sapply and purrr::map and a two-column lookup table, but it beat me. If you have suggestions, I’m eager to hear them.\n\n\nCode\n# create a lookup table for groups to remove\nremove_groups &lt;-\n  c(\"Military Forces\", \"Police Forces\", \n    \"Operations Room\", \"Opposition Rebels\", \n    \"Alliance/Named Operation\", \"Tribal\", \n    \"Military Council\", \"Communal Militias\", \n    \"Civilians\", \"SDF\", \"Islamic State\")\n\ndf_acled_actors &lt;- \n  reshaped_df |&gt;\n  mutate(\n    across(\n      actor1:assoc_actor_1,\n      ~ case_when(\n        str_detect(.x, \"Military Forces|Government|Allied Syrian\") ~ \"Military Forces\",\n        str_detect(.x, \"Unidentified Armed|Opposition Rebels|Islamist|Sunni Muslim|JSH\") ~ \"Opposition Rebels\",\n        str_detect(.x, \"Police Forces\") ~ \"Police Forces\",\n        str_detect(.x, \"HXP|QSD|YPG|YPJ|Liberation Army of Afrin|Menbij Internal|Asayish|Syriac\") ~ \"SDF\",\n        str_detect(.x, \"HTS|JFS|Jabhat Fateh al Sham\") ~ \"Hayat Tahrir al Sham\",\n        str_detect(.x, \"Hamza Division|Hamza Brigade\") ~ \"Hamza Division\", \n        str_detect(.x, \"AAS:\") ~ \"Ahrar al Sham\", \n        str_detect(.x, \"Al Sham Corps|Al Sham Division\") ~ \"Faylaq al Sham\",\n        str_detect(.x, \"HNDZ\") ~ \"Nour al Din al Zinki\",\n        str_detect(.x, \"Sharqiya Army\") ~ \"Jaysh Sharqiya\",\n        str_detect(.x, \"Liwa al Aqsa\") ~ \"Jund al Aqsa\",\n        str_detect(.x, \"FaR:\") ~ \"Faylaq al Rahman\",\n        str_detect(.x, \"JaS:\") ~ \"Levant Front\",\n        str_detect(.x, \"Sultan Suleiman Shah\") ~ \"Sultan Suleiman Shah\",\n        str_detect(.x, \"Operations Room\") ~ \"Operations Room\",\n        str_detect(.x, \"Operation Room\") ~ \"Operations Room\",\n        str_detect(.x, \"Wa Harredh al Moa'mineen\") ~ \"Operations Room\",\n        str_detect(.x, \"JWS:|JTW:|JTS:|Euphrates Shield|Peace Spring\") ~ \"Alliance/Named Operation\",\n        str_detect(.x, \"Islamic State\") ~ \"Islamic State\",\n        str_detect(.x, \"Tribal\") ~ \"Tribal\",\n        str_detect(.x, \"Military Council\") ~ \"Military Council\",\n        str_detect(.x, \"TIP:\") ~ \"Turkistan Islamic Party\",\n        str_detect(.x, \"Communal\") ~ \"Communal Militias\",\n        str_detect(.x, \"Kurdish Ethnic\") ~ \"Kurdish Ethnic Militia\",\n        str_detect(.x, \"JaT:\") ~ \"Army of the Revolutionaries\",\n        str_detect(.x, \"Sultan Murad\") ~ \"Sultan Murad\",\n        str_detect(.x, \"Ansar al Din\") ~ \"Ansar al Din\",\n        str_detect(.x, \"JOS:\") ~ \"Lions of the East\",\n        str_detect(.x, \"LAS\") ~ \"Northern Storm Brigade\",\n        str_detect(.x, \"Civilians|Protesters|Rioters|Aid Workers|Women|Farmers|Refugees|Prisoners|Journalists|\") ~ \"Civilians\",\n        TRUE ~ .x\n      ))) |&gt;\n  \n  # remove civilans, state forces or unidentified groups\n  filter(!actor1 %in% remove_groups,\n         !assoc_actor_1 %in% remove_groups,\n         actor1 != assoc_actor_1) |&gt; \n  \n  # Get the top/most groups by number of operations\n  mutate(actor1 = fct_lump(actor1, 10),\n         assoc_actor_1 = fct_lump(assoc_actor_1, 15)) |&gt; \n  filter(actor1 != \"Other\",\n         assoc_actor_1 != \"Other\")\n\n\nLet’s check out what the numbers look like for the top 10 most operationally-active groups. How many joint (aka partnered) operations does each group conduct - measured as a percentage of overall joint operations?\n\n\nCode\nlibrary(DT)\ndatatable(df_acled_actors %&gt;% \n  pivot_longer(cols = actor1:assoc_actor_1,\n               names_to = \"cols\",\n               values_to = \"Name\") %&gt;%\n  group_by(Name) %&gt;% \n  summarize(`Partnered Ops` = n()) %&gt;% \n  mutate(Percent = round(`Partnered Ops`/sum(`Partnered Ops`),2),\n         Percent = scales::percent(Percent)) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(`Partnered Ops`))) \n\n\n\n\n\n\n\nClearly, we expect Hayat Tahrir al-Sham to be a prominent player.\nNow to prep the data for graphing…\n\n\nCode\n# create nodes with count of times each actor appears in dataset\ndf_nodes &lt;- \n  df_acled_actors |&gt; \n  pivot_longer(cols = actor1:assoc_actor_1,\n               names_to = \"cols\",\n               values_to = \"Name\") |&gt; \n  group_by(Name) |&gt; \n  summarize(count = n()) |&gt; \n  ungroup() |&gt; \n  #mutate(countG = cut(count, breaks = c(-Inf, 21, 24, 43, Inf))) |&gt; \n  mutate(countG = case_when(\n    count &lt; 21 ~ \"Least\",\n    count &lt; 25 ~ \"Less\",\n    count &lt; 44 ~ \"More\",\n    TRUE ~ \"Most\"\n  ))\n\n# from original df, weight is the count of how often the two actors work together\n# igraph looks for \"from\", \"to\", and \"weight\"\ndf_edges &lt;- \n  df_acled_actors |&gt;\n  count(actor1,assoc_actor_1) |&gt; \n  rename(weight = n,\n         from = actor1,\n         to = assoc_actor_1)\n\ngraph &lt;- \n  graph_from_data_frame(\n  df_edges, \n  vertices = df_nodes)\n\n\nAnd at last, let’s generate a pairwise network plot. Let’s see who partnered with whom.\n\n\nCode\ngraphed &lt;-\n  graph |&gt; \n  ggraph(layout = 'linear', \n         circular = TRUE) +\n  ggraph::geom_edge_arc(\n    aes(alpha = weight),\n    width = 1,\n    show.legend = FALSE,\n    color = \"grey20\"\n  ) +\n  ggraph::geom_node_point(aes(color = countG)) +\n  ggraph::geom_node_label(\n    aes(label = name, \n        color = countG),\n    alpha = .75,\n    label.size = NA,\n    fill = \"#F3F3F3\", \n    size = 3,\n    repel = F,\n    fontface = \"bold\"\n  ) +\n  labs(\n    title = \"Syrian Opposition: Never go it alone!\",\n    subtitle = \"&lt;b&gt;Bolder lines&lt;/b&gt; indicate more &lt;i&gt;joint&lt;/i&gt; combat operations with that partner.\n           &lt;br&gt;Colors indicate frequency of &lt;i&gt;total&lt;/i&gt; combat operations:\n           &lt;br&gt;&lt;b style='color:black'&gt; Most Frequent&lt;/b&gt;,\n           &lt;b style='color:#450D54'&gt; More Frequent&lt;/b&gt;,\n           &lt;b style='color:#557C9B'&gt; Less Frequent&lt;/b&gt;, and \n           &lt;b style='color:#94B147'&gt;Least Frequent&lt;/b&gt;\",\n    caption = \"Data:  &lt;b&gt;'ACLED Event Data 2017-2020'&lt;/b&gt; (acleddata.com)&lt;br&gt; Visualisation by &lt;b&gt;Michael Davies&lt;/b&gt;\"\n  ) +\n  theme_void() +\n  theme(\n    # margins: top, right, bottom, and left\n    plot.margin = margin(0.7, 0.7, 0.7, 0.7, \"cm\"),\n    plot.title = element_text(size = 20, color = \"grey30\", face = \"bold\"),\n    plot.title.position = 'plot',\n    plot.subtitle = element_markdown(lineheight = 1.2),\n    plot.caption = element_markdown(size = 8),\n    plot.caption.position = 'plot',\n    plot.background = element_rect(color = NA, fill = NA),\n    legend.position = \"none\",\n  ) +\n  scale_color_manual(values = c(\"#94B147\", \"#557C9B\", \"#450D54\", \"black\")) +\n  coord_fixed(ratio = 0.6, clip = \"off\")\n\n\nlibrary(magick)\nimg &lt;- \n  image_read(\"jihadi.PNG\") |&gt;\n  image_resize(\"570x380\") |&gt;\n  image_transparent(\"grey\", fuzz = 35)\n\nggdraw() + \n  draw_plot(\n    ggplot() + \n      theme_void() + \n      theme(\n        plot.background = element_rect(color = NA, fill = \"#F3F3F3\") \n      )) +\n  draw_image(interpolate = F, \n             img, \n             scale = .35, \n             x = 0, \n             y = 0) +\n  draw_plot(graphed) \n\n\n\n\n\nAhhh the tangled web they weave. I know I’ve been mixing metaphors.\nI supposed I should have used the colors to signify ideology so that we can see if ideology and parnterships “travel well together.” However, I became interested in how the frequency of operations and partnership might reflect battlefield success. For instance, Hayat Tahrir al-Sham, by all accounts, rose to be the most dominant opposition group. This is reflected in the fact that they conducted the most operations overall. Notably, however, they seldom engaged in an operation on their own. They engaged in the most frequent “joint operations” - despite being consider hardliners, jihadists, and ISIS offshoots.4\nMaybe the key to insurgent survival is - never go it alone."
  },
  {
    "objectID": "posts/pairwise_network/network_pairwise.html#footnotes",
    "href": "posts/pairwise_network/network_pairwise.html#footnotes",
    "title": "Insurgent (Pairwise) Networks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFotini Christia, “Alliance Formation in Civil Wars”.↩︎\nI accessed the data from The Armed Conflict Location & Event Data Project (ACLED) in the spring of 2021.↩︎\nMajor Dusty Turner, U.S. Army is a monster coder and overall big brain. I’ve leaned on him for a number of problems. He consistently digs me out of a hole – all while cracking a joke.↩︎\nThe Armed Conflict Location & Event Data Project (ACLED), Actor Profile: Hayat Tahrir al-Sham (HTS) 26 July 2023.↩︎"
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html",
    "href": "posts/peacekeeping_survival/peacekeep.html",
    "title": "Parametric Survival Analysis: UN Peacekeeping Missions",
    "section": "",
    "text": "Image Source: https://peacekeeping.un.org/en/department-of-peace-operations\n\n\nThis topic has been on my mind for a while. I theorize (based on nothing but my gut) that there might be a relationship between the duration of a peacekeeping mission and the type of war that just occurred.\nIt seems plausible that civil wars have dynamics that exacerbate peacekeeping missions. Civil wars represent neighbors fighting for control of terrain and power over the respective inhabitants. Grievances commonly persist beyond the resolution of open conflict. Peace is fragile and vulnerable to spoilers, grudges, and revenge-seeking—all of which represent challenges to peacekeeping missions.\nOn the other hand, generally speaking, interstate wars might have more clearly defined objectives and could be resolved more swiftly through diplomatic means or clearly defined peace agreements, leading to comparatively shorter peacekeeping mission durations.\n\n\n\n\n\n\nIn short:\n\n\n\nWars vary widely in terms of internal and external dynamics—most of which linger as peacekeeping missions try to stabilize the situation. Is there a relationship between war type and peacekeeping mission duration?\n\n\nSo, here I will conduct a parametric event history (AKA survival) analysis, a statistical technique that unravels “time-to-event” data. The “event” in this case is the completion of the peacekeeping mission for each type of war: civil war (civil), interstate war (interst), and intrastate war (icw).\nObviously, there are several steps in this process. Even more confusing is the multitude of different models. Keeping everything straight can be a bit messy: parametric vs. non-parametric models, proportional hazards vs. accelerated failure time models, monotonic vs. non-monotonic distributions, and the various combinations.\nI grabbed a happy snap of how these models were summarized on the white board. I’ve kept this happy snap filed away.\n\n\nPH: Proportional hazards\nAFT: Accelerated Failure time\n\nIn short/general, we can say that Kaplan-Meier estimation provides descriptive survival information, while the catalog of parametric models and Cox models offer more advanced statistical analysis of survival data, allowing for hypothesis testing and the examination of covariate effects. They can be used together to provide a comprehensive understanding of survival patterns and relationships.\n\n\n\n\n\n\n\n\n\nI will fit a few parametric survival models using different distributions. Last, I’ll look at the (non-parametric) Cox proportional hazards model, which offers an alternative perspective on analyzing covariate effects.\n\n\nCode\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(flexsurv)\nlibrary(survival)\nlibrary(survminer)\nlibrary(ggsurvfit)\nlibrary(coxed)\n#library(texreg)\nsource('my_gg_theme.R')\n\n\n\n\n\n\n\n\nImportant\n\n\n\nI implemented all models in both R and Python (See respective tabs). The results, of course, are marginally different likely a result of rounding error. Therefore, I base all interpretations on the results from R for no good reason."
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html#load-clean-and-look-at-the-data",
    "href": "posts/peacekeeping_survival/peacekeep.html#load-clean-and-look-at-the-data",
    "title": "Parametric Survival Analysis: UN Peacekeeping Missions",
    "section": "Load, Clean and Look at the Data",
    "text": "Load, Clean and Look at the Data\nIn the context of implementing a survival model using flexsurvreg() in R, the appropriate shape of the dataframe depends on how you want to model the relationship between the covariates and the survival outcome. Both dataframe structures you provided are valid, but they represent different ways of specifying the covariates in the model.\n\n\nCode\nun_df &lt;- \n  read_dta('UNdata.dta') %&gt;% \n  select(failed, duration, civil, interst, icw) %&gt;%  \n  drop_na()\n\n# references\n# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5868723/\n# https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html#Kaplan-Meier_plots\n# https://lifelines.readthedocs.io/en/latest/lifelines.plotting.html\n\n# reshaping to experiment\nun_df_cat &lt;- \n  un_df %&gt;% \n  mutate(\n    wartype = factor(\n      if_else(civil == 1,\n              'civil',\n              if_else(interst == 1,\n                      'interst',\n                      'icw')))) %&gt;%\n  mutate(wartype = \n           fct_relevel(wartype,\n                       \"icw\",\n                       \"civil\",\n                       \"interst\")) %&gt;% \n  select(-c(civil, interst, icw))\n\n# Python requrres numeric\nun_df_num &lt;- \n  un_df %&gt;% \n  mutate(\n    wartype = \n      if_else(civil == 1,\n              1,\n              if_else(interst == 1,\n                      2,\n                      3))) %&gt;% \n  select(-c(civil, interst, icw))\n\nun_df %&gt;% \n  head() %&gt;% \n  knitr::kable()\n\n\n\n\n\nfailed\nduration\ncivil\ninterst\nicw\n\n\n\n\n1\n2\n0\n0\n1\n\n\n1\n4\n1\n0\n0\n\n\n1\n5\n0\n0\n1\n\n\n1\n5\n1\n0\n0\n\n\n1\n7\n0\n0\n1\n\n\n1\n7\n0\n1\n0"
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html#question-2",
    "href": "posts/peacekeeping_survival/peacekeep.html#question-2",
    "title": "Peacekeeping Missions and Event History (AKA Survival) Analysis",
    "section": "Question 2",
    "text": "Question 2\n\n\n\n\n\n\nPrompt:\n\n\n\nEstimate a parametric survival model using the generalized gamma distribution and interpret the coefficient estimates.\n\n\n\nR solutionPython Solution\n\n\nIntercept only (NULL) model:\n\n\nCode\n## Estimate parametric models - Generalized gamma\n\ngamma_fit &lt;- flexsurvreg(\n  formula = \n    Surv(time = duration,\n         event = failed) ~ 1,\n  data = un_df,\n  dist = \"gengamma\"\n  )\n\ngamma_fit \n\n\nCall:\nflexsurvreg(formula = Surv(time = duration, event = failed) ~ \n    1, data = un_df, dist = \"gengamma\")\n\nEstimates: \n       est     L95%    U95%    se    \nmu      2.921   2.326   3.516   0.304\nsigma   1.325   1.034   1.698   0.168\nQ      -1.212  -2.003  -0.422   0.403\n\nN = 54,  Events: 39,  Censored: 15\nTotal time at risk: 3994\nLog-likelihood = -197.3282, df = 3\nAIC = 400.6565\n\n\nWhen using a wide dataframe (stratified war type with cols = civil, interst, and icw), R produces:\n\n\nCode\n## Estimate parametric models - Generalized gamma\n\ngamma_fit &lt;- flexsurvreg(\n  formula = \n    Surv(time = duration,\n         event = failed) ~ civil + interst,\n  data = un_df,\n  dist = \"gengamma\"\n  )\n\ngamma_fit \n\n\nCall:\nflexsurvreg(formula = Surv(time = duration, event = failed) ~ \n    civil + interst, data = un_df, dist = \"gengamma\")\n\nEstimates: \n         data mean  est      L95%     U95%     se       exp(est)  L95%   \nmu            NA     3.0254   2.2488   3.8019   0.3962       NA        NA\nsigma         NA     1.3252   1.0380   1.6917   0.1651       NA        NA\nQ             NA    -0.9043  -1.8681   0.0595   0.4917       NA        NA\ncivil     0.2593    -0.2528  -1.1581   0.6525   0.4619   0.7766    0.3141\ninterst   0.1852     0.9711  -0.0535   1.9957   0.5228   2.6408    0.9479\n         U95%   \nmu            NA\nsigma         NA\nQ             NA\ncivil     1.9203\ninterst   7.3575\n\nN = 54,  Events: 39,  Censored: 15\nTotal time at risk: 3994\nLog-likelihood = -195.346, df = 5\nAIC = 400.692\n\n\nI reshaped to include a wartype covariate that collapses the three types of war to war column – primarily because this is the shape required for Python. Note that R can handle it either way:\n\n\nCode\nun_df_cat %&gt;% \n  head() %&gt;% \n  knitr::kable()\n\n\n\n\n\nfailed\nduration\nwartype\n\n\n\n\n1\n2\nicw\n\n\n1\n4\ncivil\n\n\n1\n5\nicw\n\n\n1\n5\ncivil\n\n\n1\n7\nicw\n\n\n1\n7\ninterst\n\n\n\n\n\n\n\nCode\n## Estimate parametric models - Generalized gamma\n\ngamma_fit &lt;- flexsurvreg(\n  formula = \n    Surv(time = duration,\n         event = failed) ~ wartype,\n  data = un_df_cat,\n  dist = \"gengamma\"\n  )\n\ngamma_fit \n\n\nCall:\nflexsurvreg(formula = Surv(time = duration, event = failed) ~ \n    wartype, data = un_df_cat, dist = \"gengamma\")\n\nEstimates: \n                data mean  est      L95%     U95%     se       exp(est)\nmu                   NA     3.0254   2.2488   3.8019   0.3962       NA \nsigma                NA     1.3252   1.0380   1.6917   0.1651       NA \nQ                    NA    -0.9043  -1.8681   0.0595   0.4917       NA \nwartypecivil     0.2593    -0.2528  -1.1581   0.6525   0.4619   0.7766 \nwartypeinterst   0.1852     0.9711  -0.0535   1.9957   0.5228   2.6408 \n                L95%     U95%   \nmu                   NA       NA\nsigma                NA       NA\nQ                    NA       NA\nwartypecivil     0.3141   1.9203\nwartypeinterst   0.9479   7.3575\n\nN = 54,  Events: 39,  Censored: 15\nTotal time at risk: 3994\nLog-likelihood = -195.346, df = 5\nAIC = 400.692\n\n\nAccelerated Failure Time Models:\nAssumption: The accelerated failure time (AFT) model assumes that the covariates have a multiplicative effect on the survival time or the time-to-event variable. In other words, the model assumes that the covariates accelerate or decelerate the time scale in a linear way.\nCivil War\nFor an AFT model, the coefficient (call it \\(\\beta_1\\)) represents the log of the time ratio associated with the covariate. In this case, the covariate civil1 is binary, and it compares the effect of being in the group civil1 (compared to the reference group civil0) on the survival time. Since the coefficient is -0.26, we would take the exponential of the coefficient (i.e., exp(-0.26)) to get the time ratio. (Time Ratio: exp(-0.26) ≈ 0.78)\nSo, wars in the group civil1 have a survival time that is approximately 0.78 times shorter (or 22% shorter) compared to wars in the reference group civil0, all other factors being equal.\nSince the coefficient is negative, it suggests that being in the civil1 group is associated with shorter survival times (an accelerating effect on the event time) compared to the reference group civil0.\nInterstate War\nFor interstate war, the time Ratio: exp(0.9711) ≈ 2.6408 indicates that wars in the group interst1 have a survival time that is approximately 2.64 times longer (or 164% longer) compared to wars in the reference group interst0, all other factors being equal.\nSince the coefficient is positive, it suggests that being in the interst1 group is associated with longer survival times (a decelerating effect on the event time) compared to the reference group interst0.\n\n\n\nA Null model\n\n\nCode\n# https://lifelines.readthedocs.io/en/latest/Survival%20Regression.htmlAC\n\ndf = r.un_df_num\ndf['Intercept'] = 1.\n\n# create parameters &lt;-&gt; covariates dict\n# The values in the dict become can be formulas, or column names in lists:\nregressors = {\n    'mu_': df.columns.difference(['failed', 'duration']),\n    'sigma_': [\"wartype\", \"Intercept\"],\n    'lambda_': 'wartype + 1',\n}\n\n# this will regress df against all 3 parameters\n# gg_model = GeneralizedGammaRegressionFitter(penalizer=1.).\\\n#     fit(df, 'duration', 'failed')\n\ngg_model = GeneralizedGammaRegressionFitter(penalizer=0.0001).\\\n    fit(df, 'duration', 'failed', regressors=regressors)\n    \ngg_model.print_summary()\n\n\n&lt;lifelines.GeneralizedGammaRegressionFitter: fitted with 54 total observations, 15 right-censored observations&gt;\n             duration col = 'duration'\n                event col = 'failed'\n                penalizer = 0.0001\n   number of observations = 54\nnumber of events observed = 39\n           log-likelihood = -196.38\n         time fit was run = 2023-08-24 01:40:16 UTC\n\n---\n                    coef  exp(coef)   se(coef)   coef lower 95%   coef upper 95%  exp(coef) lower 95%  exp(coef) upper 95%\nparam   covariate                                                                                                         \nmu_     Intercept   2.69      14.73       1.31             0.11             5.27                 1.12               193.60\n        wartype     0.10       1.11       0.57            -1.02             1.23                 0.36                 3.41\nsigma_  wartype     0.21       1.23       0.77            -1.29             1.71                 0.27                 5.53\n        Intercept  -0.21       0.81       0.27            -0.74             0.32                 0.48                 1.38\nlambda_ Intercept  -0.71       0.49       0.13            -0.97            -0.46                 0.38                 0.63\n        wartype    -0.22       0.80       0.42            -1.04             0.60                 0.35                 1.82\n\n                    cmp to     z      p   -log2(p)\nparam   covariate                                 \nmu_     Intercept     0.00  2.05   0.04       4.62\n        wartype       0.00  0.18   0.86       0.22\nsigma_  wartype       0.00  0.27   0.78       0.35\n        Intercept     0.00 -0.78   0.44       1.20\nlambda_ Intercept     0.00 -5.47 &lt;0.005      24.38\n        wartype       0.00 -0.52   0.60       0.74\n---\nAIC = 404.76\nlog-likelihood ratio test = 1.90 on 3 df\n-log2(p) of ll-ratio test = 0.75\n\n\nCumulative Hazard Rates\n\n\nCode\ngg_model.plot()\nplt.show();"
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html#a-null-model",
    "href": "posts/peacekeeping_survival/peacekeep.html#a-null-model",
    "title": "Parametric Survival Analysis: UN Peacekeeping Missions",
    "section": "A Null model",
    "text": "A Null model\n\n\nCode\n# https://lifelines.readthedocs.io/en/latest/Survival%20Regression.htmlAC\n\ndf = r.un_df_num\ndf['Intercept'] = 1.\n\n# create parameters &lt;-&gt; covariates dict\n# The values in the dict become can be formulas, or column names in lists:\nregressors = {\n    'mu_': df.columns.difference(['failed', 'duration']),\n    'sigma_': [\"wartype\", \"Intercept\"],\n    'lambda_': 'wartype + 1',\n}\n\n# this will regress df against all 3 parameters\n# gg_model = GeneralizedGammaRegressionFitter(penalizer=1.).\\\n#     fit(df, 'duration', 'failed')\n\ngg_model = GeneralizedGammaRegressionFitter(penalizer=0.0001).\\\n    fit(df, 'duration', 'failed', regressors=regressors)\n    \ngg_model.print_summary()\n\n\n&lt;lifelines.GeneralizedGammaRegressionFitter: fitted with 54 total observations, 15 right-censored observations&gt;\n             duration col = 'duration'\n                event col = 'failed'\n                penalizer = 0.0001\n   number of observations = 54\nnumber of events observed = 39\n           log-likelihood = -196.38\n         time fit was run = 2023-08-27 18:54:06 UTC\n\n---\n                    coef  exp(coef)   se(coef)   coef lower 95%   coef upper 95%  exp(coef) lower 95%  exp(coef) upper 95%\nparam   covariate                                                                                                         \nmu_     Intercept   2.69      14.73       1.31             0.11             5.27                 1.12               193.60\n        wartype     0.10       1.11       0.57            -1.02             1.23                 0.36                 3.41\nsigma_  wartype     0.21       1.23       0.77            -1.29             1.71                 0.27                 5.53\n        Intercept  -0.21       0.81       0.27            -0.74             0.32                 0.48                 1.38\nlambda_ Intercept  -0.71       0.49       0.13            -0.97            -0.46                 0.38                 0.63\n        wartype    -0.22       0.80       0.42            -1.04             0.60                 0.35                 1.82\n\n                    cmp to     z      p   -log2(p)\nparam   covariate                                 \nmu_     Intercept     0.00  2.05   0.04       4.62\n        wartype       0.00  0.18   0.86       0.22\nsigma_  wartype       0.00  0.27   0.78       0.35\n        Intercept     0.00 -0.78   0.44       1.20\nlambda_ Intercept     0.00 -5.47 &lt;0.005      24.38\n        wartype       0.00 -0.52   0.60       0.74\n---\nAIC = 404.76\nlog-likelihood ratio test = 1.90 on 3 df\n-log2(p) of ll-ratio test = 0.75\n\n\nCumulative Hazard Rates\n\n\nCode\ngg_model.plot()\nplt.show();"
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html#question-3",
    "href": "posts/peacekeeping_survival/peacekeep.html#question-3",
    "title": "Peacekeeping Missions and Event History (AKA Survival) Analysis",
    "section": "Question 3",
    "text": "Question 3\n\n\n\n\n\n\nPrompt:\n\n\n\nHere I choose one monotonic distribution and one non-monotonic distribution and estimate additional parametric survival models and interpret the results.\n\n\n\nR SolutionPython Solution\n\n\nMonotonic Distribution\n\n\nCode\n## Estimate parametric models - Generalized gamma\n\nmonot_fit &lt;- flexsurvreg(\n  formula = \n    Surv(time = duration,\n         event = failed) ~ wartype,\n  data = un_df_cat,\n  dist = \"weibull\"\n  )\n\nmonot_fit \n\n\nCall:\nflexsurvreg(formula = Surv(time = duration, event = failed) ~ \n    wartype, data = un_df_cat, dist = \"weibull\")\n\nEstimates: \n                data mean  est       L95%      U95%      se        exp(est)\nshape                 NA     0.8069    0.6331    1.0285    0.0999        NA\nscale                 NA    72.8156   43.2961  122.4616   19.3139        NA\nwartypecivil      0.2593    -1.1004   -1.9741   -0.2267    0.4458    0.3327\nwartypeinterst    0.1852     1.7368    0.5284    2.9452    0.6165    5.6793\n                L95%      U95%    \nshape                 NA        NA\nscale                 NA        NA\nwartypecivil      0.1389    0.7972\nwartypeinterst    1.6963   19.0152\n\nN = 54,  Events: 39,  Censored: 15\nTotal time at risk: 3994\nLog-likelihood = -201.1528, df = 4\nAIC = 410.3055\n\n\nNon-Monotonic Distribution\n\n\nCode\n## Estimate parametric models - Generalized gamma\n\nnon_mono_fit &lt;- flexsurvreg(\n  formula = \n    Surv(time = duration,\n         event = failed) ~ wartype,\n  data = un_df_cat,\n  dist = \"lognormal\"\n  )\n\nnon_mono_fit \n\n\nCall:\nflexsurvreg(formula = Surv(time = duration, event = failed) ~ \n    wartype, data = un_df_cat, dist = \"lognormal\")\n\nEstimates: \n                data mean  est     L95%    U95%    se      exp(est)  L95%  \nmeanlog             NA      3.592   3.081   4.103   0.261      NA        NA\nsdlog               NA      1.365   1.080   1.724   0.163      NA        NA\nwartypecivil     0.259     -0.590  -1.483   0.304   0.456   0.555     0.227\nwartypeinterst   0.185      1.385   0.319   2.451   0.544   3.995     1.376\n                U95%  \nmeanlog             NA\nsdlog               NA\nwartypecivil     1.355\nwartypeinterst  11.597\n\nN = 54,  Events: 39,  Censored: 15\nTotal time at risk: 3994\nLog-likelihood = -196.7765, df = 4\nAIC = 401.5531\n\n\n\n\nWeibull Distribution\n\n\nCode\nweibull_model = WeibullFitter().\\\n    fit(r.un_df_num['duration'], event_observed = r.un_df_num['failed'])\n\nprint(weibull_model.summary)\n\n\n              coef   se(coef)  coef lower 95%  ...         z         p   -log2(p)\nlambda_  87.564550  22.050354       44.346651  ...  3.925767  0.000086  13.497710\nrho_      0.636418   0.076044        0.487374  ... -4.781193  0.000002  19.130345\n\n[2 rows x 8 columns]\n\n\n\n\nCode\n# weibull_model.plot()\nweibull_model.plot()\nplt.show();\n\n\n\n\n\n\n\nCode\n# create an exponential model\nlogn_model = LogNormalFitter().\\\n    fit(r.un_df['duration'], event_observed = r.un_df['failed'])\n\nprint(logn_model.summary)\n\n\n            coef  se(coef)  coef lower 95%  ...          z             p    -log2(p)\nmu_     3.710521  0.220683        3.277989  ...  16.813784  1.934223e-63  208.329716\nsigma_  1.527447  0.182320        1.170107  ...   2.892978  3.816083e-03    8.033692\n\n[2 rows x 8 columns]\n\n\n\n\nCode\nlogn_model.plot()\nplt.show();"
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html#question-4",
    "href": "posts/peacekeeping_survival/peacekeep.html#question-4",
    "title": "Peacekeeping Missions and Event History (AKA Survival) Analysis",
    "section": "Question 4",
    "text": "Question 4\n\n\n\n\n\n\nPrompt:\n\n\n\nEstimate a Cox model and interpret the coefficient estimates.\n\n\n\nR SolutionPython Solution\n\n\n\n\n\nCode\n#library(gtsummary)\nsummary(cox1 &lt;- \n          coxph(Surv(time = duration,\n                     event = failed) ~ \n                  civil + interst, \n                data = un_df,\n                ties = \"efron\"))\n\n\nCall:\ncoxph(formula = Surv(time = duration, event = failed) ~ civil + \n    interst, data = un_df, ties = \"efron\")\n\n  n= 54, number of events= 39 \n\n           coef exp(coef) se(coef)      z Pr(&gt;|z|)  \ncivil    0.7561    2.1300   0.3798  1.991   0.0465 *\ninterst -0.8723    0.4180   0.5041 -1.730   0.0835 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\ncivil       2.130     0.4695    1.0118     4.484\ninterst     0.418     2.3923    0.1556     1.123\n\nConcordance= 0.619  (se = 0.042 )\nLikelihood ratio test= 9.32  on 2 df,   p=0.009\nWald test            = 8.65  on 2 df,   p=0.01\nScore (logrank) test = 9.5  on 2 df,   p=0.009\n\n\nCode\n# cox1 %&gt;% \n#   tbl_regression(exp = TRUE) \n\n# summary(cox2 &lt;- \n#           coxph(Surv(time = duration,\n#                      event = failed) ~ \n#                   wartype, \n#                 data = un_df_cat,\n#                 ties = \"efron\"))\n\n# cox3 %&gt;% \n#   tbl_regression(exp = TRUE)\n\n\nCox proportional hazards regression model is used to analyze the association between covariates and the hazard function (the risk of an event occurring at a specific time) in survival data. The Cox proportional hazards model assumes that the hazard for any individual is proportional to the hazard for any other individual at all time points. This means that the hazard ratio between two groups remains constant over time.\nAlternatively, we can think of the hazard rates obtained from the Cox proportional hazards model represent the estimated instantaneous risk of an event occurring at a particular time. More specifically, the hazard rate at a specific time represents the conditional probability that an event will occur at that time, given that the individual has survived up to that time and has the specific covariate values.\nIn the context of categorical covariates, the hazard rates obtained for different levels of the categorical variable indicate how the risk of the event changes over time compared to the reference group. A hazard rate greater than 1 indicates a higher risk (higher probability of an event occurring) relative to the reference group.\ncivil1:\n\nCoefficient (beta): 0.76\nHazard Ratio (exp(coef)): 2.13\n\nInterpretation: Individuals in the civil1 group have a hazard (risk) of experiencing the event (ending of peacekeeping mission) approximately 2.13 times higher than individuals in the reference group civil0, all other factors being equal. (The coefficient is statistically significant at the 0.05 level)\ninterst1:\n\nCoefficient (beta): -0.87\nHazard Ratio (exp(coef)): 0.42\n\nInterpretation: Individuals in the interst1 group have a hazard (risk) of experiencing the event approximately 0.42 times lower (or 58.2% lower) than individuals in the reference group interst0, all other factors being equal. (The coefficient is statistically significant at the 0.10 level)\n\n\n\n\nCode\ncph_model = CoxPHFitter()\n\ncph_model.fit(r.un_df, \n    duration_col = 'duration', \n    event_col = 'failed', \n    formula = 'civil + interst')\n\n\n&lt;lifelines.CoxPHFitter: fitted with 54 total observations, 15 right-censored observations&gt;\n\n\nCode\nprint(cph_model.summary)\n\n\n               coef  exp(coef)  se(coef)  ...         z         p  -log2(p)\ncovariate                                 ...                              \ncivil      0.756088   2.129929  0.379778  ...  1.990868  0.046495  4.426767\ninterst   -0.872107   0.418070  0.504038  ... -1.730241  0.083587  3.580575\n\n[2 rows x 11 columns]\n\n\n\n\nCode\ncph_model.plot()\nplt.show()\n\n\n\n\n\n\nQuestion 5\n\n\n\n\n\n\nPrompt:\n\n\n\nOf the four estimated models, identify the “best”-fitting model and justify your selection. Produce plots of the survival function and hazard rate based on your chosen model.\n\n\nLet’s pick the model with the lowest AIC:\n\n\nCode\ndata = {\n  \"log Normal\": [round(logn_model.AIC_,2), round(logn_model.BIC_,2)],\n  \"Weibull\": [round(weibull_model.AIC_,2), round(weibull_model.BIC_,2)],\n  \"Gen Gamma\": [round(gg_model.AIC_,2), round(gg_model.BIC_,2)]\n}\n\npy_mod_metric = pd.DataFrame(data, index = [['AIC', 'BIC']]).\\\n    rename_axis(\"Metric\").\\\n    reset_index()\n\n\n\n\nCode\nlibrary(reticulate)\n\npy$py_mod_metric %&gt;% \n  knitr::kable()\n\n\n\n\n\nMetric\nlog Normal\nWeibull\nGen Gamma\n\n\n\n\nAIC\n407.57\n423.97\n404.76\n\n\nBIC\n411.55\n427.95\n404.72"
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html#question-5",
    "href": "posts/peacekeeping_survival/peacekeep.html#question-5",
    "title": "Parametric Survival Analysis: UN Peacekeeping Missions",
    "section": "Question 5",
    "text": "Question 5\n\n\n\n\n\n\nPrompt:\n\n\n\nOf the four estimated models, identify the “best”-fitting model and justify your selection. Produce plots of the survival function and hazard rate based on your chosen model.\n\n\nLet’s pick the model with the lowest AIC:\n\n\nCode\ndata = {\n  \"log Normal\": [round(logn_model.AIC_,2), round(logn_model.BIC_,2)],\n  \"Weibull\": [round(weibull_model.AIC_,2), round(weibull_model.BIC_,2)],\n  \"Gen Gamma\": [round(gg_model.AIC_,2), round(gg_model.BIC_,2)]\n}\n\npy_mod_metric = pd.DataFrame(data, index = [['AIC', 'BIC']]).\\\n    rename_axis(\"Metric\").\\\n    reset_index()\n\n\n\n\nCode\nlibrary(reticulate)\n\npy$py_mod_metric %&gt;% \n  knitr::kable()\n\n\n\n\n\nMetric\nlog Normal\nWeibull\nGen Gamma\n\n\n\n\nAIC\n407.57\n423.97\n404.76\n\n\nBIC\n411.55\n427.95\n404.72"
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html#question-6",
    "href": "posts/peacekeeping_survival/peacekeep.html#question-6",
    "title": "Peacekeeping Missions and Event History (AKA Survival) Analysis",
    "section": "Question 6",
    "text": "Question 6\n\n\n\n\n\n\nPrompt:\n\n\n\nExplain the consequences of estimating a parametric survival model with an incorrect distribution.\n\n\nIn short, the suitability of any distribution for your data depends on how well it fits the underlying data-generating process.\nBefore implementing a model, we must give thought to the data generating process or the underlying mechanism or model that generates the observed data. We represent these processes through distributions–and the respective distributional parameters. Using an incorrect parametric distribution (Weibull for instance) when the true distribution is Gamma can result in biased parameter estimates, poor model fit, and inaccurate survival predictions (among other things) because the respective distribution parameters (shape and scale) are significantly different."
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html#appendix",
    "href": "posts/peacekeeping_survival/peacekeep.html#appendix",
    "title": "Peacekeeping Missions and Event History (AKA Survival) Analysis",
    "section": "Appendix",
    "text": "Appendix\nI grabbed a picture of how my professor organized these models. I’ve kept this happy snap filed away."
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html#war-type-and-peacekeeping-mission-duration",
    "href": "posts/peacekeeping_survival/peacekeep.html#war-type-and-peacekeeping-mission-duration",
    "title": "Parametric Survival Analysis: UN Peacekeeping Missions",
    "section": "",
    "text": "Image Source: https://peacekeeping.un.org/en/department-of-peace-operations\n\n\nThis topic has been on my mind for a while. I theorize (based on nothing but my gut) that there might be a relationship between the duration of a peacekeeping mission and the type of war that just occurred.\nIt seems plausible that civil wars have dynamics that exacerbate peacekeeping missions. Civil wars represent neighbors fighting for control of terrain and power over the respective inhabitants. Grievances commonly persist beyond the resolution of open conflict. Peace is fragile and vulnerable to spoilers, grudges, and revenge-seeking—all of which represent challenges to peacekeeping missions.\nOn the other hand, generally speaking, interstate wars might have more clearly defined objectives and could be resolved more swiftly through diplomatic means or clearly defined peace agreements, leading to comparatively shorter peacekeeping mission durations.\n\n\n\n\n\n\nIn short:\n\n\n\nWars vary widely in terms of internal and external dynamics—most of which linger as peacekeeping missions try to stabilize the situation. Is there a relationship between war type and peacekeeping mission duration?\n\n\nSo, here I will conduct a parametric event history (AKA survival) analysis, a statistical technique that unravels “time-to-event” data. The “event” in this case is the completion of the peacekeeping mission for each type of war: civil war (civil), interstate war (interst), and intrastate war (icw).\nObviously, there are several steps in this process. Even more confusing is the multitude of different models. Keeping everything straight can be a bit messy: parametric vs. non-parametric models, proportional hazards vs. accelerated failure time models, monotonic vs. non-monotonic distributions, and the various combinations.\nI grabbed a happy snap of how these models were summarized on the white board. I’ve kept this happy snap filed away.\n\n\nPH: Proportional hazards\nAFT: Accelerated Failure time\n\nIn short/general, we can say that Kaplan-Meier estimation provides descriptive survival information, while the catalog of parametric models and Cox models offer more advanced statistical analysis of survival data, allowing for hypothesis testing and the examination of covariate effects. They can be used together to provide a comprehensive understanding of survival patterns and relationships.\n\n\n\n\n\n\n\n\n\nI will fit a few parametric survival models using different distributions. Last, I’ll look at the (non-parametric) Cox proportional hazards model, which offers an alternative perspective on analyzing covariate effects.\n\n\nCode\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(flexsurv)\nlibrary(survival)\nlibrary(survminer)\nlibrary(ggsurvfit)\nlibrary(coxed)\n#library(texreg)\nsource('my_gg_theme.R')\n\n\n\n\n\n\n\n\nImportant\n\n\n\nI implemented all models in both R and Python (See respective tabs). The results, of course, are marginally different likely a result of rounding error. Therefore, I base all interpretations on the results from R for no good reason."
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html#kaplan-meier-plot",
    "href": "posts/peacekeeping_survival/peacekeep.html#kaplan-meier-plot",
    "title": "Parametric Survival Analysis: UN Peacekeeping Missions",
    "section": "Kaplan-Meier Plot",
    "text": "Kaplan-Meier Plot\nInitial (descriptive) look at the data: The survival probability reflects the likelihood of an individual surviving or not experiencing the event up to that time point. It ranges from 0 to 1, with 0 indicating no survival (event occurred) and 1 indicating complete survival (no event occurred). Each curve below represents a different group within the study, and we see distinct differences in survival probabilities between the groups.\n\nR solutionPython Solution\n\n\n\n\nCode\nsurvfit2(Surv(duration, failed) ~ 1, data = un_df) %&gt;% \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Survival probability\",\n    title = \"Peacekeeping missions: Overall survival probability\"\n  ) + \n  add_confidence_interval() +\n  add_risktable() +\n  my_gg_theme\n\n\n\n\n\nWe can use the summary() to find the probability of surviving to 1 year, which is approximately 20%. (Note: the time variable in the data is actually in days, so we need to use times = 365.25)\n\n\nCode\nsummary(survfit(\n  Surv(duration, failed) ~ 1, \n  data = un_df), \n  times = 365.25)\n\n\nCall: survfit(formula = Surv(duration, failed) ~ 1, data = un_df)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  365      3      39    0.196   0.063        0.105        0.369\n\n\n\n\nCode\nsurvfit2(Surv(duration, failed) ~ \n           civil + interst + icw, \n         data = un_df) %&gt;% \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Survival probability\",\n    title = \"Peacekeeping missions: Survival probability by Type\"\n  ) + \n  add_risktable() +\n  my_gg_theme\n\n\n\n\n\n\n\nAccording to the Python documentation, these plots show the survival function of the model plus it’s area-under-the-curve (AUC) up until the point t. The AUC is known as the restricted mean survival time (RMST).\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom lifelines import GeneralizedGammaFitter, ExponentialFitter, WeibullFitter, CoxPHFitter, LogNormalFitter\nfrom lifelines import GeneralizedGammaRegressionFitter\n\nfrom lifelines.utils import restricted_mean_survival_time\nfrom lifelines.datasets import load_waltons\nfrom lifelines.plotting import rmst_plot, plot_lifetimes\nfrom lifelines import KaplanMeierFitter\nfrom lifelines.plotting import rmst_plot\n\n\n\n\nCode\n# https://lifelines.readthedocs.io/en/latest/lifelines.plotting.html\n \ndf = r.un_df_cat\n\ntime_limit = 10\n\n# Create Kaplan-Meier fitted objects for each group\nkmf_civil = KaplanMeierFitter().\\\n    fit(df['duration'], df['failed']) # label='wartype'\n\nrmst_plot(kmf_civil, t=time_limit, show_censors=False)\n\nplt.xlabel('Time')\nplt.ylabel('RMST')\nplt.title('Overall Restricted Mean Survival Time (RMST) Plot - wide df')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\n# https://lifelines.readthedocs.io/en/latest/lifelines.plotting.html\n\ndf = r.un_df\n# Separate the data into groups based on 'civil' and 'interst'\nix_civil = df['civil'] == \"1\"\nix_interst = df['interst'] == \"1\"\nix_icw = df['icw'] == \"1\"\n\n# Create Kaplan-Meier fitted objects for each group\nkmf_civil = KaplanMeierFitter().\\\n    fit(df['duration'][ix_civil], df['failed'][ix_civil], label='Civil War')\n    \nkmf_interst = KaplanMeierFitter().\\\n    fit(df['duration'][ix_interst], df['failed'][ix_interst], label='Interstate War')\n    \nkmf_icw = KaplanMeierFitter().\\\n    fit(df['duration'][ix_icw], df['failed'][ix_icw], label='icw')\n\n# Plot RMST for each group\nax = plt.subplot(111)\nrmst_plot(kmf_civil, t=10, ax=ax, show_censors=False)\nrmst_plot(kmf_interst, t=10, ax=ax, show_censors=False)\nrmst_plot(kmf_icw, t=10, ax=ax, show_censors=False)\n\nplt.xlabel('Time')\nplt.ylabel('RMST')\nplt.title('Restricted Mean Survival Time (RMST) Plot')\nplt.legend()\nplt.show()\n\n\n\n\nCode\ndf = r.un_df_cat\n# Separate the data into groups based on 'civil' and 'interst'\nix_civil = df['wartype'] == \"civil\"\nix_interst = df['wartype'] == \"interst\"\nix_icw = df['wartype'] == \"icw\"\ntime_limit = 10\n\n# Create Kaplan-Meier fitted objects for each group\nkmf_civil = KaplanMeierFitter().\\\n    fit(df['duration'][ix_civil], df['failed'][ix_civil], label='Civil War')\n    \nkmf_interst = KaplanMeierFitter().\\\n    fit(df['duration'][ix_interst], df['failed'][ix_interst], label='Interstate War')\n    \nkmf_icw = KaplanMeierFitter().\\\n    fit(df['duration'][ix_icw], df['failed'][ix_icw], label='icw')\n\n# Plot RMST for each group\nax = plt.subplot(111)\nrmst_plot(kmf_civil, t=time_limit, ax=ax, show_censors=False)\nrmst_plot(kmf_interst, t=time_limit, ax=ax, show_censors=False)\nrmst_plot(kmf_icw, t=time_limit, ax=ax, show_censors=False)\n\nplt.xlabel('Time')\nplt.ylabel('RMST')\nplt.title('Restricted Mean Survival Time (RMST) Plot')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html#fit-the-model",
    "href": "posts/peacekeeping_survival/peacekeep.html#fit-the-model",
    "title": "Parametric Survival Analysis: UN Peacekeeping Missions",
    "section": "Fit the model",
    "text": "Fit the model\n\n\n\n\n\n\nPrompt:\n\n\n\nEstimate a parametric survival model using the generalized gamma distribution and interpret the coefficient estimates.\n\n\n\nR solutionPython Solution\n\n\nIntercept only (NULL) model:\n\n\nCode\n## Estimate parametric models - Generalized gamma\n\ngamma_fit &lt;- flexsurvreg(\n  formula = \n    Surv(time = duration,\n         event = failed) ~ 1,\n  data = un_df,\n  dist = \"gengamma\"\n  )\n\ngamma_fit \n\n\nCall:\nflexsurvreg(formula = Surv(time = duration, event = failed) ~ \n    1, data = un_df, dist = \"gengamma\")\n\nEstimates: \n       est     L95%    U95%    se    \nmu      2.921   2.326   3.516   0.304\nsigma   1.325   1.034   1.698   0.168\nQ      -1.212  -2.003  -0.422   0.403\n\nN = 54,  Events: 39,  Censored: 15\nTotal time at risk: 3994\nLog-likelihood = -197.3282, df = 3\nAIC = 400.6565\n\n\nWhen using a wide dataframe (stratified war type with cols = civil, interst, and icw), R produces:\n\n\nCode\n## Estimate parametric models - Generalized gamma\n\ngamma_fit &lt;- flexsurvreg(\n  formula = \n    Surv(time = duration,\n         event = failed) ~ civil + interst,\n  data = un_df,\n  dist = \"gengamma\"\n  )\n\ngamma_fit \n\n\nCall:\nflexsurvreg(formula = Surv(time = duration, event = failed) ~ \n    civil + interst, data = un_df, dist = \"gengamma\")\n\nEstimates: \n         data mean  est      L95%     U95%     se       exp(est)  L95%   \nmu            NA     3.0254   2.2488   3.8019   0.3962       NA        NA\nsigma         NA     1.3252   1.0380   1.6917   0.1651       NA        NA\nQ             NA    -0.9043  -1.8681   0.0595   0.4917       NA        NA\ncivil     0.2593    -0.2528  -1.1581   0.6525   0.4619   0.7766    0.3141\ninterst   0.1852     0.9711  -0.0535   1.9957   0.5228   2.6408    0.9479\n         U95%   \nmu            NA\nsigma         NA\nQ             NA\ncivil     1.9203\ninterst   7.3575\n\nN = 54,  Events: 39,  Censored: 15\nTotal time at risk: 3994\nLog-likelihood = -195.346, df = 5\nAIC = 400.692\n\n\nI reshaped to include a wartype covariate that collapses the three types of war to war column – primarily because this is the shape required for Python. Note that R can handle it either way:\n\n\nCode\nun_df_cat %&gt;% \n  head() %&gt;% \n  knitr::kable()\n\n\n\n\n\nfailed\nduration\nwartype\n\n\n\n\n1\n2\nicw\n\n\n1\n4\ncivil\n\n\n1\n5\nicw\n\n\n1\n5\ncivil\n\n\n1\n7\nicw\n\n\n1\n7\ninterst\n\n\n\n\n\n\n\nCode\n## Estimate parametric models - Generalized gamma\n\ngamma_fit &lt;- flexsurvreg(\n  formula = \n    Surv(time = duration,\n         event = failed) ~ wartype,\n  data = un_df_cat,\n  dist = \"gengamma\"\n  )\n\ngamma_fit \n\n\nCall:\nflexsurvreg(formula = Surv(time = duration, event = failed) ~ \n    wartype, data = un_df_cat, dist = \"gengamma\")\n\nEstimates: \n                data mean  est      L95%     U95%     se       exp(est)\nmu                   NA     3.0254   2.2488   3.8019   0.3962       NA \nsigma                NA     1.3252   1.0380   1.6917   0.1651       NA \nQ                    NA    -0.9043  -1.8681   0.0595   0.4917       NA \nwartypecivil     0.2593    -0.2528  -1.1581   0.6525   0.4619   0.7766 \nwartypeinterst   0.1852     0.9711  -0.0535   1.9957   0.5228   2.6408 \n                L95%     U95%   \nmu                   NA       NA\nsigma                NA       NA\nQ                    NA       NA\nwartypecivil     0.3141   1.9203\nwartypeinterst   0.9479   7.3575\n\nN = 54,  Events: 39,  Censored: 15\nTotal time at risk: 3994\nLog-likelihood = -195.346, df = 5\nAIC = 400.692\n\n\nAccelerated Failure Time Models:\nAssumption: The accelerated failure time (AFT) model assumes that the covariates have a multiplicative effect on the survival time or the time-to-event variable. In other words, the model assumes that the covariates accelerate or decelerate the time scale in a linear way.\nCivil War\nFor an AFT model, the coefficient (call it \\(\\beta_1\\)) represents the log of the time ratio associated with the covariate. In this case, the covariate civil1 is binary, and it compares the effect of being in the group civil1 (compared to the reference group civil0) on the survival time. Since the coefficient is -0.26, we would take the exponential of the coefficient (i.e., exp(-0.26)) to get the time ratio. (Time Ratio: exp(-0.26) ≈ 0.78)\nSo, wars in the group civil1 have a survival time that is approximately 0.78 times shorter (or 22% shorter) compared to wars in the reference group civil0, all other factors being equal.\nSince the coefficient is negative, it suggests that being in the civil1 group is associated with shorter survival times (an accelerating effect on the event time) compared to the reference group civil0.\nInterstate War\nFor interstate war, the time Ratio: exp(0.9711) ≈ 2.6408 indicates that wars in the group interst1 have a survival time that is approximately 2.64 times longer (or 164% longer) compared to wars in the reference group interst0, all other factors being equal.\nSince the coefficient is positive, it suggests that being in the interst1 group is associated with longer survival times (a decelerating effect on the event time) compared to the reference group interst0.\n\n\n\nA Null model\n\n\nCode\n# https://lifelines.readthedocs.io/en/latest/Survival%20Regression.htmlAC\n\ndf = r.un_df_num\ndf['Intercept'] = 1.\n\n# create parameters &lt;-&gt; covariates dict\n# The values in the dict become can be formulas, or column names in lists:\nregressors = {\n    'mu_': df.columns.difference(['failed', 'duration']),\n    'sigma_': [\"wartype\", \"Intercept\"],\n    'lambda_': 'wartype + 1',\n}\n\n# this will regress df against all 3 parameters\n# gg_model = GeneralizedGammaRegressionFitter(penalizer=1.).\\\n#     fit(df, 'duration', 'failed')\n\ngg_model = GeneralizedGammaRegressionFitter(penalizer=0.0001).\\\n    fit(df, 'duration', 'failed', regressors=regressors)\n    \ngg_model.print_summary()\n\n\n&lt;lifelines.GeneralizedGammaRegressionFitter: fitted with 54 total observations, 15 right-censored observations&gt;\n             duration col = 'duration'\n                event col = 'failed'\n                penalizer = 0.0001\n   number of observations = 54\nnumber of events observed = 39\n           log-likelihood = -196.38\n         time fit was run = 2023-08-27 18:54:06 UTC\n\n---\n                    coef  exp(coef)   se(coef)   coef lower 95%   coef upper 95%  exp(coef) lower 95%  exp(coef) upper 95%\nparam   covariate                                                                                                         \nmu_     Intercept   2.69      14.73       1.31             0.11             5.27                 1.12               193.60\n        wartype     0.10       1.11       0.57            -1.02             1.23                 0.36                 3.41\nsigma_  wartype     0.21       1.23       0.77            -1.29             1.71                 0.27                 5.53\n        Intercept  -0.21       0.81       0.27            -0.74             0.32                 0.48                 1.38\nlambda_ Intercept  -0.71       0.49       0.13            -0.97            -0.46                 0.38                 0.63\n        wartype    -0.22       0.80       0.42            -1.04             0.60                 0.35                 1.82\n\n                    cmp to     z      p   -log2(p)\nparam   covariate                                 \nmu_     Intercept     0.00  2.05   0.04       4.62\n        wartype       0.00  0.18   0.86       0.22\nsigma_  wartype       0.00  0.27   0.78       0.35\n        Intercept     0.00 -0.78   0.44       1.20\nlambda_ Intercept     0.00 -5.47 &lt;0.005      24.38\n        wartype       0.00 -0.52   0.60       0.74\n---\nAIC = 404.76\nlog-likelihood ratio test = 1.90 on 3 df\n-log2(p) of ll-ratio test = 0.75\n\n\nCumulative Hazard Rates\n\n\nCode\ngg_model.plot()\nplt.show();"
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html#estimate-a-cox-model",
    "href": "posts/peacekeeping_survival/peacekeep.html#estimate-a-cox-model",
    "title": "Parametric Survival Analysis: UN Peacekeeping Missions",
    "section": "Estimate a Cox model",
    "text": "Estimate a Cox model\n\n\n\n\n\n\nEstimate a Cox model\n\n\n\nEstimate a Cox model and interpret the coefficient estimates.\n\n\n\nR SolutionPython Solution\n\n\n\n\n\nCode\n#library(gtsummary)\nsummary(cox1 &lt;- \n          coxph(Surv(time = duration,\n                     event = failed) ~ \n                  civil + interst, \n                data = un_df,\n                ties = \"efron\"))\n\n\nCall:\ncoxph(formula = Surv(time = duration, event = failed) ~ civil + \n    interst, data = un_df, ties = \"efron\")\n\n  n= 54, number of events= 39 \n\n           coef exp(coef) se(coef)      z Pr(&gt;|z|)  \ncivil    0.7561    2.1300   0.3798  1.991   0.0465 *\ninterst -0.8723    0.4180   0.5041 -1.730   0.0835 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\ncivil       2.130     0.4695    1.0118     4.484\ninterst     0.418     2.3923    0.1556     1.123\n\nConcordance= 0.619  (se = 0.042 )\nLikelihood ratio test= 9.32  on 2 df,   p=0.009\nWald test            = 8.65  on 2 df,   p=0.01\nScore (logrank) test = 9.5  on 2 df,   p=0.009\n\n\nCode\n# cox1 %&gt;% \n#   tbl_regression(exp = TRUE) \n\n# summary(cox2 &lt;- \n#           coxph(Surv(time = duration,\n#                      event = failed) ~ \n#                   wartype, \n#                 data = un_df_cat,\n#                 ties = \"efron\"))\n\n# cox3 %&gt;% \n#   tbl_regression(exp = TRUE)\n\n\nCox proportional hazards regression model is used to analyze the association between covariates and the hazard function (the risk of an event occurring at a specific time) in survival data. The Cox proportional hazards model assumes that the hazard for any individual is proportional to the hazard for any other individual at all time points. This means that the hazard ratio between two groups remains constant over time.\nAlternatively, we can think of the hazard rates obtained from the Cox proportional hazards model represent the estimated instantaneous risk of an event occurring at a particular time. More specifically, the hazard rate at a specific time represents the conditional probability that an event will occur at that time, given that the individual has survived up to that time and has the specific covariate values.\nIn the context of categorical covariates, the hazard rates obtained for different levels of the categorical variable indicate how the risk of the event changes over time compared to the reference group. A hazard rate greater than 1 indicates a higher risk (higher probability of an event occurring) relative to the reference group.\ncivil1:\n\nCoefficient (beta): 0.76\nHazard Ratio (exp(coef)): 2.13\n\nInterpretation: Individuals in the civil1 group have a hazard (risk) of experiencing the event (ending of peacekeeping mission) approximately 2.13 times higher than individuals in the reference group civil0, all other factors being equal. (The coefficient is statistically significant at the 0.05 level)\ninterst1:\n\nCoefficient (beta): -0.87\nHazard Ratio (exp(coef)): 0.42\n\nInterpretation: Individuals in the interst1 group have a hazard (risk) of experiencing the event approximately 0.42 times lower (or 58.2% lower) than individuals in the reference group interst0, all other factors being equal. (The coefficient is statistically significant at the 0.10 level)\n\n\n\n\nCode\ncph_model = CoxPHFitter()\n\ncph_model.fit(r.un_df, \n    duration_col = 'duration', \n    event_col = 'failed', \n    formula = 'civil + interst')\n\n\n&lt;lifelines.CoxPHFitter: fitted with 54 total observations, 15 right-censored observations&gt;\n\n\nCode\nprint(cph_model.summary)\n\n\n               coef  exp(coef)  se(coef)  ...         z         p  -log2(p)\ncovariate                                 ...                              \ncivil      0.756088   2.129929  0.379778  ...  1.990868  0.046495  4.426767\ninterst   -0.872107   0.418070  0.504038  ... -1.730241  0.083587  3.580575\n\n[2 rows x 11 columns]\n\n\n\n\nCode\ncph_model.plot()\nplt.show()\n\n\n\n\n\n\nQuestion 5\n\n\n\n\n\n\nPrompt:\n\n\n\nOf the four estimated models, identify the “best”-fitting model and justify your selection. Produce plots of the survival function and hazard rate based on your chosen model.\n\n\nLet’s pick the model with the lowest AIC:\n\n\nCode\ndata = {\n  \"log Normal\": [round(logn_model.AIC_,2), round(logn_model.BIC_,2)],\n  \"Weibull\": [round(weibull_model.AIC_,2), round(weibull_model.BIC_,2)],\n  \"Gen Gamma\": [round(gg_model.AIC_,2), round(gg_model.BIC_,2)]\n}\n\npy_mod_metric = pd.DataFrame(data, index = [['AIC', 'BIC']]).\\\n    rename_axis(\"Metric\").\\\n    reset_index()\n\n\n\n\nCode\nlibrary(reticulate)\n\npy$py_mod_metric %&gt;% \n  knitr::kable()\n\n\n\n\n\nMetric\nlog Normal\nWeibull\nGen Gamma\n\n\n\n\nAIC\n407.57\n423.97\n404.76\n\n\nBIC\n411.55\n427.95\n404.72"
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html#consequences-of-an-incorrect-distribution",
    "href": "posts/peacekeeping_survival/peacekeep.html#consequences-of-an-incorrect-distribution",
    "title": "Parametric Survival Analysis: UN Peacekeeping Missions",
    "section": "Consequences of an incorrect distribution",
    "text": "Consequences of an incorrect distribution\n\n\n\n\n\n\nDistributions\n\n\n\nUnderstand the consequences of estimating a parametric survival model with an incorrect distribution.\n\n\nIn short, the suitability of any distribution for your data depends on how well it fits the underlying data-generating process.\nBefore implementing a model, we must give thought to the data generating process or the underlying mechanism or model that generates the observed data. We represent these processes through distributions–and the respective distributional parameters. Using an incorrect parametric distribution (Weibull for instance) when the true distribution is Gamma can result in biased parameter estimates, poor model fit, and inaccurate survival predictions (among other things) because the respective distribution parameters (shape and scale) are significantly different."
  },
  {
    "objectID": "posts/peacekeeping_survival/peacekeep.html#monotonic-and-non-monotonic-distributions",
    "href": "posts/peacekeeping_survival/peacekeep.html#monotonic-and-non-monotonic-distributions",
    "title": "Parametric Survival Analysis: UN Peacekeeping Missions",
    "section": "Monotonic and non-monotonic distributions",
    "text": "Monotonic and non-monotonic distributions\n\n\n\n\n\n\nPrompt:\n\n\n\nHere I choose one monotonic distribution and one non-monotonic distribution and estimate additional parametric survival models and interpret the results.\n\n\n\nR SolutionPython Solution\n\n\nMonotonic Distribution\n\n\nCode\n## Estimate parametric models - Generalized gamma\n\nmonot_fit &lt;- flexsurvreg(\n  formula = \n    Surv(time = duration,\n         event = failed) ~ wartype,\n  data = un_df_cat,\n  dist = \"weibull\"\n  )\n\nmonot_fit \n\n\nCall:\nflexsurvreg(formula = Surv(time = duration, event = failed) ~ \n    wartype, data = un_df_cat, dist = \"weibull\")\n\nEstimates: \n                data mean  est       L95%      U95%      se        exp(est)\nshape                 NA     0.8069    0.6331    1.0285    0.0999        NA\nscale                 NA    72.8156   43.2961  122.4616   19.3139        NA\nwartypecivil      0.2593    -1.1004   -1.9741   -0.2267    0.4458    0.3327\nwartypeinterst    0.1852     1.7368    0.5284    2.9452    0.6165    5.6793\n                L95%      U95%    \nshape                 NA        NA\nscale                 NA        NA\nwartypecivil      0.1389    0.7972\nwartypeinterst    1.6963   19.0152\n\nN = 54,  Events: 39,  Censored: 15\nTotal time at risk: 3994\nLog-likelihood = -201.1528, df = 4\nAIC = 410.3055\n\n\nNon-Monotonic Distribution\n\n\nCode\n## Estimate parametric models - Generalized gamma\n\nnon_mono_fit &lt;- flexsurvreg(\n  formula = \n    Surv(time = duration,\n         event = failed) ~ wartype,\n  data = un_df_cat,\n  dist = \"lognormal\"\n  )\n\nnon_mono_fit \n\n\nCall:\nflexsurvreg(formula = Surv(time = duration, event = failed) ~ \n    wartype, data = un_df_cat, dist = \"lognormal\")\n\nEstimates: \n                data mean  est     L95%    U95%    se      exp(est)  L95%  \nmeanlog             NA      3.592   3.081   4.103   0.261      NA        NA\nsdlog               NA      1.365   1.080   1.724   0.163      NA        NA\nwartypecivil     0.259     -0.590  -1.483   0.304   0.456   0.555     0.227\nwartypeinterst   0.185      1.385   0.319   2.451   0.544   3.995     1.376\n                U95%  \nmeanlog             NA\nsdlog               NA\nwartypecivil     1.355\nwartypeinterst  11.597\n\nN = 54,  Events: 39,  Censored: 15\nTotal time at risk: 3994\nLog-likelihood = -196.7765, df = 4\nAIC = 401.5531\n\n\n\n\nWeibull Distribution\n\n\nCode\nweibull_model = WeibullFitter().\\\n    fit(r.un_df_num['duration'], event_observed = r.un_df_num['failed'])\n\nprint(weibull_model.summary)\n\n\n              coef   se(coef)  coef lower 95%  ...         z         p   -log2(p)\nlambda_  87.564550  22.050354       44.346651  ...  3.925767  0.000086  13.497710\nrho_      0.636418   0.076044        0.487374  ... -4.781193  0.000002  19.130345\n\n[2 rows x 8 columns]\n\n\n\n\nCode\n# weibull_model.plot()\nweibull_model.plot()\nplt.show();\n\n\n\n\n\n\n\nCode\n# create an exponential model\nlogn_model = LogNormalFitter().\\\n    fit(r.un_df['duration'], event_observed = r.un_df['failed'])\n\nprint(logn_model.summary)\n\n\n            coef  se(coef)  coef lower 95%  ...          z             p    -log2(p)\nmu_     3.710521  0.220683        3.277989  ...  16.813784  1.934223e-63  208.329716\nsigma_  1.527447  0.182320        1.170107  ...   2.892978  3.816083e-03    8.033692\n\n[2 rows x 8 columns]\n\n\n\n\nCode\nlogn_model.plot()\nplt.show();"
  },
  {
    "objectID": "posts/bayes_invasion/bayes blog post.html",
    "href": "posts/bayes_invasion/bayes blog post.html",
    "title": "Bayes and the Battlefield",
    "section": "",
    "text": "Predicting rare events is hard.\nConsider geopolitical risk or national security research questions such as:\nThese questions are hard because they combine ambiguity, sparse data, and cognitive traps.\nIf only there were a straightforward approach that could manage the ambiguity, address the sparse data, and avoid the cognitive traps.\nThat’s precisely where Bayesian thinking shines—offering a principled, transparent way to reason under uncertainty in contexts like these."
  },
  {
    "objectID": "posts/bayes_invasion/bayes blog post.html#the-core-idea",
    "href": "posts/bayes_invasion/bayes blog post.html#the-core-idea",
    "title": "Bayes and the Battlefield",
    "section": "💡 The Core Idea",
    "text": "💡 The Core Idea\nThe logic of Bayes Theorem is built on the definition of conditional probability, which helps us find the probability of event \\(A\\) happening given that event \\(B\\) has already occurred: the probability of \\(A\\) given \\(B\\) is the probability of \\(A\\) and \\(B\\) divided by the probability of \\(B\\).\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nSo, the national security question is now formulated as:\nWhat is the probability of Russia invading Ukraine by early 2022 given observed Russian troop movements to the border?\n\n\n\n\n\n\nDeriving Bayes’ Theorem (Drop down if you dare!)\n\n\n\n\n\nIt’s actually not too bad.\nBecause \\(P(A \\cap B) = P(B \\cap A)\\), we can flip the conditional:\n\\[\nP(A \\cap B) = P(B \\mid A) \\cdot P(A)\n\\]\nSubstitute into the original formula:\n\\[\nP(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n\\]\nThis is Bayes’ Theorem. It allows you to reason backward—from a known likelihood \\(P(B \\mid A)\\) and prior \\(P(A)\\), to an updated belief \\(P(A \\mid B)\\).\n\n🔁 Summary Visual Mapping\n\n\n\n\n\n\n\n\nConcept\nEquation\nVisual Idea\n\n\n\n\nConditional Probability\n\\(P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\\)\nWhat portion of \\(B\\) also satisfies \\(A\\)\n\n\nJoint from Conditional\n\\(P(A \\cap B) = P(B \\mid A) \\cdot P(A)\\)\nFlip the lens: assume \\(A\\) first\n\n\nBayes’ Theorem\n\\(P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\\)\nInvert the condition using prior and likelihood\n\n\n\n\n\n\n\nWith some rearranging and substitution, we get Bayes’ Theorem. Stating Bayes’ Rule in plain terms and more formally:\n\\[\n\\begin{aligned}\n\\textbf{Bayes' Rule (Plain Terms):} \\quad\n\\text{Posterior} &= \\frac{\\text{Prior} \\times \\text{Likelihood}}{\\text{Evidence}}\\\\[3ex]\n\\textbf{Bayes' Rule (Formal):} \\quad\nP(H_i \\mid E) &= \\frac{P(H_i) \\cdot P(E \\mid H_i)}{\\sum_j P(H_j) \\cdot P(E \\mid H_j)}\n\\end{aligned}\n\\]\nThis reads as: The probability of a hypothesis given the new evidence (posterior) is equal to our belief before seeing the data (prior), multiplied by how likely the evidence is under that hypothesis (likelihood), divided by the total probability of observing the evidence across all hypotheses (normalizing constant). And that is a mouthful.\nSo, we have four parts:\n\nThe prior — your belief about the hypothesis before seeing the new data (the baseline).\nThe likelihood — how likely the observed evidence is, assuming each hypothesis is true.\nThe normalizing constant (or marginal likelihood) — the probability of the evidence across all hypotheses.\nThe posterior — your updated belief after accounting for the evidence."
  },
  {
    "objectID": "posts/bayes_invasion/bayes blog post.html#summary-visual-mapping",
    "href": "posts/bayes_invasion/bayes blog post.html#summary-visual-mapping",
    "title": "Bayes and the Battlefield",
    "section": "🔁 Summary Visual Mapping",
    "text": "🔁 Summary Visual Mapping\n\n\n\n\n\n\n\n\nConcept\nEquation\nVisual Idea\n\n\n\n\nConditional Probability\n\\(P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\\)\nWhat portion of \\(B\\) also satisfies \\(A\\)\n\n\nJoint from Conditional\n\\(P(A \\cap B) = P(B \\mid A) \\cdot P(A)\\)\nFlip the lens: assume \\(A\\) first\n\n\nBayes’ Theorem\n\\(P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\\)\nInvert the condition using prior and likelihood"
  },
  {
    "objectID": "posts/bayes_invasion/bayes blog post.html#a-worked-example-will-russia-invade-ukraine",
    "href": "posts/bayes_invasion/bayes blog post.html#a-worked-example-will-russia-invade-ukraine",
    "title": "Bayes and the Battlefield",
    "section": "🧮 A Worked Example: Will Russia Invade Ukraine?",
    "text": "🧮 A Worked Example: Will Russia Invade Ukraine?\nImagine it’s fall 2021 and the world is watching Russia getting a little more squirrelly in Ukraine since its initial invasion in 2014. You consider three competing hypotheses:\n\n\\(H_1\\): Invasion - Russia will invade Ukraine by early 2022\n\\(H_2\\): Brinkmanship - Russia escalates tension but stops short\n\\(H_3\\): De-escalation - Russia de-escalates\n\n\nThe Set-Up: Prior, Likelihood and Normalizing Constant\n\nThe Prior\nThe prior is your best estimate of each hypothesis’s likelihood, based on what you knew before the new evidence appeared. This step is often the most difficult and contentious step in Bayesian analysis: How do we establish a prior probability - particularly when we often have already seen the new evidence? (This is important — the priors across all hypotheses must sum to 1.)\nOne way to estimate a prior is to ask:\nIn similar geopolitical conditions, how often did these scenarios result in invasion? For instance, history might show 3 “invasions” to 7 “non-invasions” under these same conditions.\nSo, we might say our prior is equal to 3 / 10 or 0.30, giving us \\(H_1\\): Russia will invade Ukraine by early 2022 of 0.30.\n\n\n\n\n\n\nTip\n\n\n\nThe logical implication is that there is a 0.7 probability that one or more alternative options can occur. If we’re just interested in how likely Russia is to invade, it might be sufficient to carry out the analysis with \\(H_2\\) = 0.7 that Russia will not invade.\nHowever, we can make the question a little more demanding. So, here we evaluate three hypotheses.\n\n\nAfter reviewing historical data and hashing it out among analysts, we settle on the following priors:\n\n\\(P(H_1) = 0.30\\) - Invasion\n\\(P(H_2) = 0.50\\) - Brinkmanship\n\\(P(H_3) = 0.20\\) - De-escalation\n\nThen, new evidence \\(( E_1 )\\) appears: We obtain imagery of Russian troop movements massing in proximity of the Ukraine border.\n\n\nThe likelihood\nWe now assess how likely it is that we would observe this data under each scenario. One by one, how likely is it that we’d observe this data (troop movements to the border) under each hypothesis? Let’s say the above evidence favors \\(H_1\\): Russia will invade Ukraine by early 2022, and we arrive at:\n\n\\(P(E_1 \\mid H_1) = 0.8\\) – Invasion: Troop buildups and supply chains are consistent with imminent invasion.\n\\(P(E_1 \\mid H_2) = 0.6\\) – Strategic brinkmanship: Some military signaling and maneuvers fit escalation without full commitment.\n\\(P(E_1 \\mid H_3) = 0.1\\) – De-escalation: Large troop movements are hard to reconcile with genuine de-escalation.\n\n\n\n\n\n\n\nWhat Is Likelihood?\n\n\n\n\n\nLikelihood is easy to misunderstand. It’s not the probability of the hypothesis — it’s the probability of the evidence, assuming the hypothesis is true.\nSo when we say\n\\(P(E_1 \\mid H_1) = 0.8\\),\nwe’re saying: If invasion is the true hypothesis, how likely is it that we’d see this troop movement?\nBayesian updating hinges on comparing these likelihoods across all competing hypotheses. A likelihood on its own tells us little — the real insight comes from how well different hypotheses explain the same piece of evidence.\nIt’s also important to distinguish between evidence and likelihood. The evidence is the observation itself — for example, satellite images of troop buildups. The likelihood is our estimate of how probable that observation is under each hypothesis.\n\nEvidence — the actual data we observe (e.g., troop movements, diplomatic breakdowns).\n\nLikelihood — a model-based judgment about how well each hypothesis explains that evidence.\n\nThis distinction matters: we don’t directly calculate the probability of a hypothesis from the evidence. Instead, we assess how well each hypothesis accounts for the evidence — and then update our beliefs accordingly using Bayes’ Rule.\n\n\n\nTo get the numerator for each hypothesis, we simply multiply each prior by each likelihood:\n\\[\n\\begin{aligned}\nP(H_1) \\cdot P(E_1 \\mid H_1) &= 0.30 \\times 0.8 = 0.24 \\\\\nP(H_2) \\cdot P(E_1 \\mid H_2) &= 0.50 \\times 0.6 = 0.30 \\\\\nP(H_3) \\cdot P(E_1 \\mid H_3) &= 0.20 \\times 0.1 = 0.02 \\\\\n\\end{aligned}\n\\]\nAfter completing this step, we can already see which hypothesis is most probable. However, for the sake of completeness, let’s continue…\n\n\nDenominator:\nWe compute the total probability of this first piece of evidence \\(( E_1 )\\) by summing over all hypotheses:\n\\[\n\\begin{aligned}\nP(E_1) &= \\sum_{j=1}^{3} P(H_j) \\cdot P(E_1 \\mid H_j) \\\\\n     &= P(H_1) \\cdot P(E_1 \\mid H_1) + P(H_2) \\cdot P(E_1 \\mid H_2) + P(H_3) \\cdot P(E_1 \\mid H_3)\\\\\n     &= (0.30 \\times 0.80) + (0.50 \\times 0.60) + (0.20 \\times 0.10) \\\\\n     &= 0.24 + 0.30 + 0.02 \\\\\n     &= 0.56\n\\end{aligned}\n\\]\n\n\n\n🧮 Evaluate the Posterior under each hypothesis\nPlug and play to find our updated belief(s).\n\\[\n\\begin{aligned}\nP(H_1 \\mid E_1) &= \\frac{0.24}{0.56} \\approx 0.429 -\\text{ Probability of Invasion} \\\\\nP(H_2 \\mid E_1) &= \\frac{0.30}{0.56} \\approx 0.536 -\\text{ Probability of Brinksmanship}\\\\\nP(H_3 \\mid E_1) &= \\frac{0.02}{0.56} \\approx 0.036 -\\text{ Probability of De-escalation}\\\\\n\\end{aligned}\n\\]\n\n\nDecision trees help visualize this process\n\n\n\n\nflowchart LR\n  A{{\"Initial State\"}}\n\n  subgraph Priors\n    B(\"P(Invasion) = 0.30\")\n    C(\"P(Brinksmanship) = 0.50\")\n    D(\"P(De-escalation) = 0.20\")\n  end\n\n  subgraph Likelihoods\n    E(\"P(Troop buildup | Invasion) = 0.8\")\n    F(\"P(Troop buildup | Brinksmanship) = 0.6\")\n    G(\"P(Troop buildup | De-escalation) = 0.1\")\n  end\n\n  subgraph Posteriors\n    H(\"P(Invasion | Troop buildup) = (0.30 × 0.8) / 0.56 = 0.429\")\n    I(\"P(Brinksmanship | Troop buildup) = (0.50 × 0.6) / 0.56 = 0.536\")\n    J(\"P(De-escalation | Troop buildup) = (0.20 × 0.1) / 0.56 = 0.036\")\n  end\n\n  A --&gt; B\n  A --&gt; C\n  A --&gt; D\n  B --&gt; E --&gt; H\n  C --&gt; F --&gt; I\n  D --&gt; G --&gt; J\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral Probability Tree\n\n\n\n\n\nTo keep things focused and digestible, this example follows the path of observed evidence — that is, how likely each hypothesis makes the actual evidence we’ve seen.\nBut in a complete Bayesian model, we could (and sometimes should) also consider the complement of the evidence: what if the data hadn’t shown up? What if troop movements weren’t observed?\n\n\n\nSo, after seeing the new data:\n\nThe chance of invasion increases from 30% to 43%\n\nThe chance of continued tension remains most likely at 54%\n\nThe chance of de-escalation falls to just 4%\n\n\n\nWhy It Matters\nThis is how we update beliefs in light of evolving evidence. Starting from our priors, we revise the relative probabilities in light of the new evidence. While the probability of invasion has increased, the evidence still supports continued tension as the most likely scenario.\nNote: Bayesian reasoning doesn’t claim certainty — but it makes your assumptions explicit and your updates principled.\nWe’ll now explore how beliefs dynamically evolve sequentially as new events unfold…\n\n\n\n\n\n\nEvents leading up to the invasion\n\n\n\n\n\nSequential Updates Before the Invasion\nBayesian analysis is well-suited for tracking how beliefs shift over time in response to new information. Below is a timeline of key events from late 2021 to early 2022 that could be used as informational updates when estimating the probability of a Russian invasion of Ukraine.\nEach event serves as a potential data point \\(( E_t )\\) in a sequential Bayesian update process.\n\n📅 Significant Events Prior to Russia’s Full-Scale Invasion\n\n\n\n\n\n\n\n\nDate\nEvent Description\nPotential Bayesian Impact\n\n\n\n\nApr–May 2021\nFirst major Russian troop buildup near Ukraine border\nEarly warning; raises baseline invasion probability\n\n\nNov 10, 2021\nU.S. satellite imagery confirms renewed Russian troop buildup\nEvidence of sustained planning; updates likelihood for invasion\n\n\nDec 7, 2021\nBiden–Putin virtual summit; Biden warns of sanctions\nDiplomatic signaling; some chance for deterrence\n\n\nDec 17, 2021\nRussia issues sweeping security demands to NATO and U.S.\nRaises stakes; suggests maximalist aims\n\n\nJan 10–13, 2022\nU.S.–Russia and NATO–Russia talks collapse without progress\nRemoves peaceful resolution path; posterior moves toward H1\n\n\nJan 14, 2022\nUkraine suffers major cyberattack (attributed to Russia)\nGray zone escalation; supports H1 and H2\n\n\nJan 18, 2022\nRussian troops begin arriving in Belarus for “exercises”\nExpands invasion axes; increases likelihood under H1\n\n\nJan 25, 2022\nU.S. and NATO formally reject Russia’s security proposals\nTriggers rhetorical escalation; diplomacy effectively dead\n\n\nFeb 10–20, 2022\nRussia–Belarus joint military exercises near Ukrainian border\nPre-positioning for multi-front attack\n\n\nFeb 15–16, 2022\nRussia announces some troop withdrawals (but U.S. says false)\nConflicting signals; short-term belief in H2 may briefly rise\n\n\nFeb 17, 2022\nSharp increase in shelling in Donbas (blamed on Ukraine by Russia)\nProxy provocation; tactical signal under H1\n\n\nFeb 21, 2022\nPutin recognizes Donetsk & Luhansk “independence” and sends in “peacekeepers”\nDe facto invasion begins; major update toward certainty of H1\n\n\nFeb 24, 2022\nFull-scale invasion of Ukraine begins\nEvent confirmed; H1 = 1; H2 and H3 collapse\n\n\n\n\n\n🔄 Using This Timeline for Bayesian Updating\n\nTreat each row as a new evidence ( E_t )\n\nEstimate how likely each event is under competing hypotheses:\n\n\\(( H_1 )\\): Russia will invade\n\n\\(( H_2 )\\): Russia will escalate without invasion\n\n\\(( H_3 )\\): Russia will de-escalate\n\n\nUse Bayes’ Rule to update your belief at each step:\n\n\\[\nP(H_i \\mid E_t) = \\frac{P(H_i) \\cdot P(E_t \\mid H_i)}{\\sum_j P(H_j) \\cdot P(E_t \\mid H_j)}\n\\]\nThe timeline above is one way to structure your priors and likelihoods in a transparent, iterative way."
  },
  {
    "objectID": "posts/bayes_invasion/bayes blog post.html#sequential-updates-before-the-invasion",
    "href": "posts/bayes_invasion/bayes blog post.html#sequential-updates-before-the-invasion",
    "title": "Bayes and the Battlefield",
    "section": "Sequential Updates Before the Invasion",
    "text": "Sequential Updates Before the Invasion\nBayesian analysis is well-suited for tracking how beliefs shift over time in response to new information. Below is a timeline of key events from late 2021 to early 2022 that could be used as informational updates when estimating the probability of a Russian invasion of Ukraine.\nEach event serves as a potential data point \\(( E_t )\\) in a sequential Bayesian update process.\n\n📅 Significant Events Prior to Russia’s Full-Scale Invasion\n\n\n\n\n\n\n\n\nDate\nEvent Description\nPotential Bayesian Impact\n\n\n\n\nApr–May 2021\nFirst major Russian troop buildup near Ukraine border\nEarly warning; raises baseline invasion probability\n\n\nNov 10, 2021\nU.S. satellite imagery confirms renewed Russian troop buildup\nEvidence of sustained planning; updates likelihood for invasion\n\n\nDec 7, 2021\nBiden–Putin virtual summit; Biden warns of sanctions\nDiplomatic signaling; some chance for deterrence\n\n\nDec 17, 2021\nRussia issues sweeping security demands to NATO and U.S.\nRaises stakes; suggests maximalist aims\n\n\nJan 10–13, 2022\nU.S.–Russia and NATO–Russia talks collapse without progress\nRemoves peaceful resolution path; posterior moves toward H1\n\n\nJan 14, 2022\nUkraine suffers major cyberattack (attributed to Russia)\nGray zone escalation; supports H1 and H2\n\n\nJan 18, 2022\nRussian troops begin arriving in Belarus for “exercises”\nExpands invasion axes; increases likelihood under H1\n\n\nJan 25, 2022\nU.S. and NATO formally reject Russia’s security proposals\nTriggers rhetorical escalation; diplomacy effectively dead\n\n\nFeb 10–20, 2022\nRussia–Belarus joint military exercises near Ukrainian border\nPre-positioning for multi-front attack\n\n\nFeb 15–16, 2022\nRussia announces some troop withdrawals (but U.S. says false)\nConflicting signals; short-term belief in H2 may briefly rise\n\n\nFeb 17, 2022\nSharp increase in shelling in Donbas (blamed on Ukraine by Russia)\nProxy provocation; tactical signal under H1\n\n\nFeb 21, 2022\nPutin recognizes Donetsk & Luhansk “independence” and sends in “peacekeepers”\nDe facto invasion begins; major update toward certainty of H1\n\n\nFeb 24, 2022\nFull-scale invasion of Ukraine begins\nEvent confirmed; H1 = 1; H2 and H3 collapse\n\n\n\n\n\n🔄 Using This Timeline for Bayesian Updating\n\nTreat each row as a new evidence ( E_t )\n\nEstimate how likely each event is under competing hypotheses:\n\n\\(( H_1 )\\): Russia will invade\n\n\\(( H_2 )\\): Russia will escalate without invasion\n\n\\(( H_3 )\\): Russia will de-escalate\n\n\nUse Bayes’ Rule to update your belief at each step:\n\n\\[\nP(H_i \\mid E_t) = \\frac{P(H_i) \\cdot P(E_t \\mid H_i)}{\\sum_j P(H_j) \\cdot P(E_t \\mid H_j)}\n\\]\nThe timeline above is one way to structure your priors and likelihoods in a transparent, iterative way."
  },
  {
    "objectID": "posts/bayes_invasion/bayes blog post.html#sequential-update-early-2022",
    "href": "posts/bayes_invasion/bayes blog post.html#sequential-update-early-2022",
    "title": "Bayes and the Battlefield",
    "section": "🔄 Sequential Update: Early 2022",
    "text": "🔄 Sequential Update: Early 2022\nBy January 2022, the situation had evolved further.\nYou observe a second body of evidence \\(( E_2 )\\):\n\nRussia moves even more armored divisions forward\n\nBelarus hosts joint military exercises\n\nRussian media prepares the domestic population\n\n\nTaking this together as one body of new evidence, this constitutes a strong signal of escalation. We now reassess the likelihoods under each hypothesis.\n\nUpdated Inputs\n\nThe Prior\nYour prior for this second round is the posterior from the first update:\n\\[\n\\begin{aligned}\nP(H_1) &= 0.429 \\\\\nP(H_2) &= 0.536 \\\\\nP(H_3) &= 0.036 \\\\\n\\end{aligned}\n\\]\n\n\nThe Likelihood\nYou now estimate how likely the new data \\(( E_2 )\\) would be under each hypothesis:\n\\[\n\\begin{aligned}\nP(E_2 \\mid H_1) &= 0.85 \\text{ Strong signal of imminent invasion.}\\\\\nP(E_2 \\mid H_2) &= 0.50 \\text{ Strong risk of miscalculation if simply signalling.}\\\\\nP(E_2 \\mid H_3) &= 0.05 \\text{ Now very unlikely.}\\\\\n\\end{aligned}\n\\]\n\n\n\n🧮 Bayes’ Rule Again\nNote the sequential updating.\n\\[\\begin{aligned}\n\\text{First update:} \\quad\nP(H_i \\mid E_1) &= \\frac{P(H_i) \\cdot P(E_1 \\mid H_i)}{\\sum_j P(H_j) \\cdot P(E_1 \\mid H_j)} \\\\[1em]\n\n\\text{Second update:} \\quad\nP(H_i \\mid E_1, E_2) &= \\frac{P(H_i \\mid E_1) \\cdot P(E_2 \\mid H_i)}{\\sum_j P(H_j \\mid E_1) \\cdot P(E_2 \\mid H_j)}\n\\end{aligned}\\]\n\n\nNumerator Calculations:\n\\[\n\\begin{aligned}\nP(H_1) \\cdot P(E_2 \\mid H_1) &= 0.429 \\times 0.85 = 0.36465 \\\\\nP(H_2) \\cdot P(E_2 \\mid H_2) &= 0.536 \\times 0.50 = 0.26800 \\\\\nP(H_3) \\cdot P(E_2 \\mid H_3) &= 0.036 \\times 0.05 = 0.00180 \\\\\n\\end{aligned}\n\\]\n\n\nDenominator (Normalization Constant):\n\\[\nP(E_2) = 0.36465 + 0.26800 + 0.00180 = 0.63445\n\\]\n\n\n✅ Updated Posterior Probabilities:\n\\[\n\\begin{aligned}\nP(H_1 \\mid E_2) &= \\frac{0.36465}{0.63445} \\approx 0.5747 \\\\\nP(H_2 \\mid E_2) &= \\frac{0.26800}{0.63445} \\approx 0.4223 \\\\\nP(H_3 \\mid E_2) &= \\frac{0.00180}{0.63445} \\approx 0.0028 \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nWhy We Always Include a Prior\n\n\n\n\n\nEvery Bayesian analysis begins with a prior — your belief or baseline estimate before seeing new data. This isn’t a technicality; it’s fundamental to rational inference.\nIf you focus only on new evidence and ignore the prior, you risk overreacting to noisy or ambiguous signals. Without anchoring to a baseline, your belief can swing wildly with each new data point — a kind of whiplash effect in your reasoning.\nIn geopolitical risk, this is especially dangerous. For example, troop movements may look alarming, but if historical data shows many similar buildups ended without conflict, then the prior probability of invasion should remain low — until the evidence becomes strong enough to warrant an update.\nIncorporating the prior ensures:\n\nStability: updates are proportional, not impulsive\n\nTransparency: your assumptions are visible and testable\n\nContext: new evidence is interpreted relative to what’s already known\n\nThe prior doesn’t bias the analysis — it grounds it.\n\n\n\n\n\nLet’s visualize where we are\n\n\n\n\nflowchart LR\n  A{{\"Subsequent State\"}}\n\n  %% Stage: Priors (from first update)\n  B(\"P(Invasion) = 0.429\")\n  C(\"P(Brinksmanship) = 0.536\")\n  D(\"P(De-escalation) = 0.036\")\n\n  %% Stage: New Likelihoods (E₂)\n  B --&gt; E(\"P(E₂ | Invasion) = 0.85\")\n  C --&gt; F(\"P(E₂ | Brinksmanship) = 0.5\")\n  D --&gt; G(\"P(E₂ | De-escalation) = 0.05\")\n\n  %% Stage: Updated Posteriors\n  E --&gt; H(\"P(Invasion | E₂) = (0.429 × 0.85) / 0.63445 ≈ 0.575\")\n  F --&gt; I(\"P(Brinksmanship | E₂) = (0.536 × 0.5) / 0.63445 ≈ 0.422\")\n  G --&gt; J(\"P(De-escalation | E₂) = (0.036 × 0.05) / 0.63445 ≈ 0.003\")\n\n  %% Connect root to priors\n  A --&gt; B\n  A --&gt; C\n  A --&gt; D\n\n\n\n\n\n\n\nInterpretation\nAfter the second round of evidence:\n\nThe belief in invasion climbs to 57%\n\nTension without invasion is now less likely, at 42%\n\nDe-escalation is nearly ruled out at 0.3%\n\nThe probability of war is no longer just plausible — it’s approaching probable.\nThis is how Bayes allows us to track changing odds in real time, rather than overreacting or sticking rigidly to outdated assumptions.\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\nBayesian reasoning provides a structured way to update beliefs as new evidence arrives.\nThe prior grounds your estimate in historical context or baseline judgment.\nLikelihoods reflect how well the evidence fits each hypothesis — but only when compared across competing hypotheses.\nThe posterior is not a guess — it’s a disciplined update of your belief.\nThis method is especially powerful for rare, ambiguous, and high-stakes geopolitical events.\n\n\n\n\n\n\n\nThe Normalizing Constant (Denominator)\nThe denominator is the scope or context of the research question. It ensures beliefs are updated relative to the universe of plausible alternatives, not in isolation. While quantitative analysts may later simplify this step, I find it pedagogically useful to retain the full expression, especially in an analytical setting."
  },
  {
    "objectID": "posts/bayes_invasion/bayes blog post.html#posterior-calculation-applying-bayes-rule",
    "href": "posts/bayes_invasion/bayes blog post.html#posterior-calculation-applying-bayes-rule",
    "title": "Bayes and the Battlefield",
    "section": "Posterior Calculation (Applying Bayes’ Rule)",
    "text": "Posterior Calculation (Applying Bayes’ Rule)\nTo evaluate our first hypothesis, the framework is:\n\\[\n\\begin{aligned}\nP(E) &= \\sum_{j=1}^{3} P(H_j) \\cdot P(E \\mid H_j) \\\\\n     &= P(H_1) \\cdot P(E \\mid H_1) + P(H_2) \\cdot P(E \\mid H_2) + P(H_3) \\cdot P(E \\mid H_3)\n\\end{aligned}\n\\]\n\n\n\n\n\n\nIsn’t Assigning Numbers a False Precision?\n\n\n\nIt’s a fair question — and a common critique.\nBayesian analysis does not claim that these numbers are objectively true or uniquely correct. Instead, it forces us to be explicit about our assumptions.\nAssigning probabilities isn’t about claiming certainty — it’s about creating a transparent, testable framework for reasoning under uncertainty. Even rough or subjective probabilities are better than vague intuition, because they can be:\n\nChallenged\n\nUpdated\n\nCompared\n\nCrucially, assigning numbers helps ensure our reasoning is logically consistent. It prevents us from believing contradictory things or shifting our judgments arbitrarily — especially when stakes are high or evidence is ambiguous. Quantification creates a structure that reveals when our beliefs are out of sync with our own logic.\nIn fact, the point of Bayesian reasoning is not to pretend we know the answer — it’s to expose our reasoning and show how it changes as new information arrives.\nThis isn’t precision for its own sake. It’s accountability for your beliefs."
  }
]